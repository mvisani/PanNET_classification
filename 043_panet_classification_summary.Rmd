---
title: "043_panet_classification_summary"
author: "Marco Visani"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    code_folding: hide
---


# Introduction

Data received was of RGChannelSetExtended formal class. NOOB was run for both 450 and EPIC data. In order to include as much data as possible, low p value probes and low bead count probes were also included. Legacy probes were removed (total of 997 probes). The probes were sorted by probe name. Data was converted to beta values. Taking the intersect between the probe names of EPIC and 450K technology and the then data was joined together. Beta values were then normalized using `champ.norm`function. 

Normalized beta values were converted back to M values. Data was adjusted for batch effect on the `Slide` using ComBat. Values were then converted back to beta values. 

QC was then performed (see below).  

# Quality control 
Surrogate variables, PCA and t-SNE were checked. 

## Check for surrogate variables
First we checked for surrogate variables. Some were found but on the variables that we are interested in. If some of the surrogate variables were found on the `Technology` variable for example, this should be a concern. Here, there is nothing to worry about. 
```{r}
rm(list = ls())
meta_data <- read.table("./data/meta_data/training_meta_data.txt", sep = "\t", header = T)
if(!exists("meth_combat_M"))
  meth_combat_M <- readRDS("./data/results/meth_combat_M.Rds")
if(!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/meth_combat_beta.Rds"))
if (!exists("meth_norm"))
  meth_norm <- as.data.frame(readRDS("./data/results/meth_normalized.Rds"))
```

```{r, fig.width = 14}
load(file = "./data/results/surrogate_var.RData")
require(ggplot2)
require(dplyr)
#this part will tell if the surrogate variables are descibing other varibales of the dataset
methSV <- as.data.frame(meth_sva$sv)
colnames(methSV) <- paste0("S", colnames(methSV))
methSV <- cbind(meta_data,methSV) 

SVList = as.list(rep(NA, times = sum(grepl("SV", colnames(methSV)))))
for (sv in seq_along(SVList)){
  SVList[[sv]] = lapply(colnames(meta_data), function(var){
    if(is.numeric(methSV[[var]]))
      test_res = summary(lm(as.formula(paste0("SV", sv,"~", var)), data = methSV))$coef[2,4]
    else
      test_res = kruskal.test(as.formula(paste0("SV", sv,"~", var)), data = methSV)$p.value
    data.frame(variable = var,
               pval = test_res,
               SV = paste0("SV", sv))
  })
}

#This part plot the 
bind_rows(SVList) %>% 
  mutate(pval_bin = cut(-log10(pval), 
                        breaks = c(0, -log10(0.05), 2, 5, 10, 300), 
                        labels = c("a", "b", "c", "d", "e"), 
                        include.lowest = T), SV = factor(SV, levels=paste0("SV", 1:42))) %>% 
  ggplot(aes(x = SV, y = variable, fill = pval_bin)) + 
  geom_tile() + 
  coord_fixed() + 
  scale_fill_manual(values = c("white", "pink", "orange", "red", "darkred"),
                    limits = c("a", "b", "c", "d", "e"),
                    labels = c("p >= 0.05", "p < 0.05", "p < 0.01", 
                               expression("p < 1x" ~ 10^{-5}), 
                               expression("p < 1x" ~ 10^{-10}))) +
  theme_classic() + 
  theme(legend.title = element_blank(), 
        legend.text.align = 0, 
        legend.background = element_rect(fill = "grey90"), 
        axis.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
  labs(title = "SVs vs biological variables")
```

Since we have many surrogate variables, there is nothing to worry about on the `Array` variable because the p-value is at 0.01 so there is a small percentage of chance that that SV was just assigned by chance. 

## PCA
PCA tries to preserve the global structure of data i.e when converting d-dimensional data to d'-dimensional data then it tries to map all the clusters as a whole due to which local structures might get lost. That is why we also checked with t-SNE. 
```{r, fig.height=10, fig.width=14}
require(fafreqs)
require(tibble)
#probevar <- rowVars(meth_combat_M)
probevar <- apply(meth_combat_M, 1, var)
probevar <- order(probevar, decreasing = T)

meth_combat_pca <- meth_combat_M[probevar[1:30000],meta_data$Sample_Name] #30000 is an arbitrary number --> look at paper on brain tumors (they took 32000)

meth_combat_pca <- t(meth_combat_pca)
pca_result <- prcomp(meth_combat_pca)

pca_df = pca_result$x %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample_Name")
pca_df = left_join(pca_df, 
                   meta_data,
                   by = "Sample_Name")
pca_eig = factoextra::get_eigenvalue(pca_result)



cowplot::plot_grid(
  ggplot(pca_df, aes(color=Technology, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MEN1, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=DAXX_ATRX, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=Grade, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MCT4_max, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=CC_Epi_newLRO, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  ncol =2, align = "v" 
)

```

We can't see any pattern. Maybe a small one for the Intermidiate_ADM probes classified by the `CC_Epi_newLRO` (bottom left image). 
The two first PCs only describe in total 21\% of the variance, which is quite low.

## t-SNE
```{r, fig.height=10, fig.width=14}
require(Rtsne)
require(RColorBrewer)
set.seed(123) # setting a fixed seed allows reproducibility 
tsne <- Rtsne::Rtsne(meth_combat_pca, theta=0.0, PCA=F,
                     max_iter=2500) #with default perplexity of 30. Theta set to 0.0 for higher accuracy (but lower speed)
                                    # PCA and max_iter are set as in Capper et al. paper
tsne_df <- tsne$Y %>% as.data.frame() %>% rownames_to_column("Sample_Name")
tsne_df$Sample_Name <- c(rownames(meth_combat_pca))
tsne_df <- left_join(tsne_df, meta_data, by="Sample_Name")


cowplot::plot_grid(
  ggplot(tsne_df, aes(x=V1, y=V2, color=Technology))+
  xlab("tSNE1") + ylab("tSNE2")+
    geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=CC_Epi_newLRO))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MEN1))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=DAXX_ATRX))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=Grade))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MCT4_max))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), ncol =2, align = "v"
)
```
Again there seems to be no significant cluster. Maybe Beta-like samples are a little bit isolated 

```{r quantification of nromalization effect}

getNormQual = function(norm_data,
                       raw_data){
  # this function is taken from the wateRmelon package (qual)
  # The na.rm = T settings are removed to have the function give an error if there are NAs
  # difference between normalized and raw
  stopifnot(all.equal(colnames(norm_data), colnames(raw_data)))
  dif  = norm_data - raw_data
  # root mean square deviation
  rmsd = sqrt(colMeans(dif^2))
  sdd  = apply(dif, 2, sd)
  sadd = apply(abs(dif), 2, sd)
  srms = rmsd/sdd
  data.frame(Sample_Name = colnames(norm_data),
             rmsd,
             sdd,
             sadd,
             srms)
}
 
```

 

```{r normalization violence, fig.height = 3.5, fig.width = 4.5}
norm_qual <- getNormQual(meth_norm, meth_combat_beta)
#detach("package:factoextra", unload=TRUE)
require(ggrepel)
require(ggplot2)
cowplot::plot_grid(
  left_join(meta_data,
            norm_qual,
            by = "Sample_Name") %>%
    ggplot(aes(x = rmsd,
               y = sdd,
               label = Sample_Name,
               color= Technology)) +
    geom_point(size = 3,
               alpha = 0.75) +
    geom_text_repel() +
    scale_color_brewer(palette = "Set1") +
    theme_classic() +
    labs(title = "NOOB + BMIQ")
)

```

This is a final "check" to see if the technology used could have a bias in our data. Here we see that the EPIC has the same trend as the 450K indicating that there shouldn't be any bias (for now...). 

# Classifier development
The random forest was developed by training the data on the 10'000 most important probes. 10'000 trees were developped, and we looked at the 




# Classifier cross-validation
Batch effect was NOT taken into account here !
```{r,echo=FALSE,message=FALSE,error=FALSE,warning=FALSE,fig.height=4,fig.width=12}
rm(list=ls())
library(pROC)
library(HandTill2001)
source(file.path("scripts", "R","multi_brier.R"))
source(file.path("scripts", "R","multi_logloss.R"))

load(file.path("results","CVresults.RData"))

AUCscores <- HandTill2001::auc(multcap(response=as.factor(y),predicted=scores))
AUCprobs <-  HandTill2001::auc(multcap(response=as.factor(y),predicted=probs))

errs <- sum(y!=ys)/length(y)
errp <- sum(y!=yp)/length(y)

briers <- brier(scores,y)
brierp <- brier(probs,y) 

logls <- mlogloss(scores,y)
loglp <- mlogloss(probs,y) 

out <- cbind(c(AUCscores,AUCprobs),c(errs,errp),c(briers,brierp),c(logls,loglp))
colnames(out) <- c("auc","misclassification","brier score","logloss")
rownames(out) <- c("raw scores","calibrated scores")

y.true <- y
y.score <- ys
y.calibrated <- yp
```

## Consensus clustering 
```{r}
require(pheatmap)
cons_cluster <- readRDS(file = "./data/raw_data/220609_NDD.consClust_DMP.Rds")
cons_cluster <- cons_cluster[[5]]
```


```{r}
plotCCheatmap = function(ccObject, metaData, previousCC = NULL, colorList){
  ccHmap = ccObject$consensusMatrix
  ccDist = ccObject$consensusTree
  ccDist_rev = ccDist
  ccDist_rev$order = rev(ccDist_rev$order)
  
  attr(ccHmap, "dimnames") = list(names(ccObject$consensusClass),
                                names(ccObject$consensusClass))
  
  metaData = as.data.frame(metaData)
  if(all.equal(rownames(metaData), as.character(seq_len(nrow(metaData)))))
    rownames(metaData) = metaData$Sample_Name
  metaData = metaData %>% 
    dplyr::select(-Sample_Name) 
  if(!is.null(previousCC))
    metaData[[previousCC]][is.na(metaData[[previousCC]])] = "new_sample"
  
  pheatmap(ccHmap,
           scale = "none",
           color = colorRampPalette(c("white", "blue"))(100),
           cluster_rows = ccDist_rev,
           cluster_cols = ccDist,
           show_rownames = F,
           show_colnames = F,
           cellheight = 2,
           cellwidth = 2,
           annotation_col = metaData,
           annotation_colors = colorList)
}
```

```{r heatmap 5 clusters, fig.height = 10, fig.width = 14}
require(dplyr)
meta_data <- read.table(file = "./data/meta_data/training_meta_data_new.txt", sep = "\t", header = T)

scores_plot <- scores[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(scores_plot)[apply(scores_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(scores_plot)[apply(scores_plot, 1, which.max)]")] <- "rf"

#now for the calibrated scores
probs_plot <- probs[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(probs_plot)[apply(probs_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(probs_plot)[apply(probs_plot, 1, which.max)]")] <- "rf_calibrated"

meta_data['groups'] <- !is.na(meta_data$Cell_type_groups)
meta_data$groups[meta_data$groups == TRUE] <- "UIU"
meta_data$groups[grep("^aP", meta_data$Sample_Name)] <- "aP"
meta_data$groups[meta_data$groups==FALSE] <- "Chan"

meta_data["corretly_classified"] <- meta_data$CC_Epi_newLRO==meta_data$rf_calibrated

max_score <- apply(probs[meta_data$Sample_Name, ], 1, max)
meta_data <- cbind(meta_data, max_score)


plotCCheatmap(cons_cluster, 
              metaData = meta_data %>% 
                dplyr::select(Sample_Name, 
                              DAXX_ATRX, 
                              MEN1, 
                              CC_Epi_newLRO,
                              rf,
                              rf_calibrated,
                              max_score),

              colorList = list(DAXX_ATRX = c(wt = "white", mut = "goldenrod4"),
                               MEN1 = c(wt = "white", mut = "goldenrod1"),
                               ConsClus_k4_UB_UCL_ICGC_Chan = c(Alpha_like = "dodgerblue4",
                                                                Beta_like = "green",
                                                                Intermediate_ADM = "lightskyblue1",
                                                                Intermediate_WT = "darksalmon",
                                                                new_sample = "yellow"),
                               CC_Epi_newLRO = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               rf = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"), 
                               rf_calibrated = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               max_score = colorRampPalette(c("white", "blue"))(100)))



```

The consensus cluster was used to "create" a response to train the random forest algorithm. The image above shows were the samples were classified according to the consensus cluster, the random forest and the random forest after calibration. 



```{r}
require(kableExtra)
table(meta_data$groups,meta_data$corretly_classified) %>% kbl %>% kable_styling()
```

Table shows the number of correctly classified and wrongly classified of the original cohorts. 

## Confusion matrix
Questions: 

* Is the consensus clutering and good response ? 
* Should the wrongly classified samples be looked again clinically ? reanalyzed ? 
* ... 

After training of the classifier with the top 10'000 probes, the calibrated random forest algorithm classified the samples as follows. 
```{r}
require(pheatmap)

load(file = "./results/rf.pred.RData")

m <-table(y.true, y.calibrated)
m <- m/apply(m, 1, sum)

pheatmap(m,
         display_numbers = table(y.true, y.calibrated),
         angle_col = 45,
         treeheight_row = 0,
         treeheight_col = 0,
         cluster_rows = F,
         cluster_cols = F,
         color=colorRampPalette(c("white", "red"))(100))
```

Shows confusion matrix as heatmap. Numbers in the cells indicates the number of samples counted in that cell. The color indicates the percentage of samples correctly identified in that class. 

Should be read for right to left: there are 6 Intermidiate_ADM that were classified in the Alpha_like class. 

Looking at the 


```{r}
require(pROC)
require(ggplot2)
roc_resp <- as.numeric(y.true!=y.calibrated)
predictor_roc <- apply(probs, 1, max)
roc_to_plot <- roc(response=roc_resp, predictor=predictor_roc, ci=TRUE, of="se")
```
```{r ROC}
roc_with_ci <- function(obj) {
  ciobj <- obj$ci
  dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                       lower = ciobj[, 1],
                       upper = ciobj[, 3])
  
  ggroc(obj) +
    theme_minimal() +
    geom_abline(
      slope = 1,
      intercept = 1,
      linetype = "dashed",
      alpha = 0.7,
      color = "black"
    ) + coord_equal() +
    geom_ribbon(
      data = dat.ci,
      aes(x = x, ymin = lower, ymax = upper),
      fill = "steelblue",
      alpha = 0.3
    ) + ggtitle(paste0("AUC : ", obj$auc))
} 

roc_with_ci(roc_to_plot)
```


```{r}
require(kableExtra)
out %>% kbl %>% kable_styling()
```





```{r}
require(ggplot2)
require(ggridges)
raw_score <- as.data.frame(scores)
raw_score["predicted"] <- as.vector(y.score)
raw_score["CC_Epi_newLRO"] <- as.vector(y)
raw_score <- raw_score[raw_score$predicted == raw_score$CC_Epi_newLRO, ]

yl <- c()
for (i in 1:nrow(raw_score)) {
  x <- raw_score[i, 'predicted']
  yl <- append(yl, raw_score[i, x])
}
raw_score["max"] <- yl
rm(yl, x)

# plot the raw scores
raw <- ggplot(raw_score, aes(x = max, y = CC_Epi_newLRO, fill = CC_Epi_newLRO)) +
  ggtitle("Raw scores for correctly classified cases") +
  xlim(0,1.2) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

```{r}
require(ggplot2)
require(ggridges)
calibrated_score <- as.data.frame(probs)
calibrated_score["predicted"] <- as.vector(y.calibrated)
calibrated_score["CC_Epi_newLRO"] <- as.vector(y)
calibrated_score <- calibrated_score[calibrated_score$predicted == calibrated_score$CC_Epi_newLRO, ]

yl <- c()
for (i in 1:nrow(calibrated_score)) {
  x <- calibrated_score[i, 'predicted']
  yl <- append(yl, calibrated_score[i, x])
}
calibrated_score["max"] <- yl
rm(yl, x)

# plot the raw scores
calibrated <- ggplot(calibrated_score, aes(x = max, y = CC_Epi_newLRO, fill = CC_Epi_newLRO)) +
  xlim(0,1.2) +
  ggtitle("Calibrated scores for correctly classified cases") +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```



```{r, fig.width=10}
cowplot::plot_grid(
  raw, 
  calibrated,
  ncol = 1)

```


# Probes  in DiDomenico paper
```{r}
require(readxl)
alpha_vs_beta <-  read_xlsx(path="./data/raw_data/DiDomenico_2020_T6_alpha_like_v_beta_like.xlsx",
                           skip = 2)
alpha_vs_beta <- alpha_vs_beta[,1]
colnames(alpha_vs_beta) <- "probes"

alpha_vs_intermidiate <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T7_alpha_like_v_intermediate.xlsx",
                                   skip = 2)
alpha_vs_intermidiate <- alpha_vs_intermidiate[,1]
colnames(alpha_vs_intermidiate) <- "probes"

intermidiate_vs_beta <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T8_intermediate_v_beta_like.xlsx",
                                  skip = 2)
intermidiate_vs_beta <- intermidiate_vs_beta[,1]
colnames(intermidiate_vs_beta) <- "probes"
```

```{r}
#require(dplyr)
#probes <- as.data.frame(rownames(importance(rf.pred, type=1)))
#colnames(probes) <- "probes"
#intersect(alpha_vs_beta, probes)
#intersect(alpha_vs_intermidiate, probes)
#intersect(intermidiate_vs_beta, probes)

```

```{r}
#imp.meandecrease <- importance(rf.pred, type=1)
#or <- order(imp.meandecrease,decreasing=T)
#
#imp.meandecrease <-  as.data.frame(rownames(imp.meandecrease[or[1:2000], ,drop=FALSE]))
#colnames(imp.meandecrease) <- "probes"
#
#intersect(alpha_vs_beta, imp.meandecrease)
```

# Cross validation on fewer probes

After having followed the protocol of Capper et al. (https://doi.org/10.1038/nature26000), the question arose of knowing if 10'000 probes were to many and could risk an over fitting. The plot below shows the mean decrease in accuracy of the ranked probes vs the rank of the probe. We can clearly see that taking 10'000 probes is too much for our dataset and we need to reduce the amount of probes taken. 

A vertical line was drawn at the 1'000 and 2'000 probes mark. 
```{r mean decrease in accuracy}
require(randomForest)
load(file = "./results/rf.pred.RData")
probes <- importance(rf.pred, type = 1)
or <- order(probes,decreasing=T)
probes <- probes[order(probes, decreasing = T)]
plot(x=c(1:length(probes)),
     #log="x",
     probes,
     cex=0.3,
     ylab="Mean decrease in accuracy",
     xlab="Probes ranked")
abline(v=c(1000, 2000), col=c("blue", "red"), lty=c(2,2))
abline(h=0)
```

 A cross validation on 2'000 and 1'000 probes was tested:


![image](./results/CV_10000_plot.png)

![image](./results/CV_2000_plot.png)
![image](./results/CV_1000_plot.png)

# 2'000 probes Classifier cross-validation 
Batch effect was NOT taken into account here !
```{r,echo=FALSE,message=FALSE,error=FALSE,warning=FALSE,fig.height=4,fig.width=12}
rm(list=ls())
library(pROC)
library(HandTill2001)
source(file.path("scripts", "R","multi_brier.R"))
source(file.path("scripts", "R","multi_logloss.R"))

load(file.path("results","CVresults_2000.RData"))

AUCscores <- HandTill2001::auc(multcap(response=as.factor(y),predicted=scores))
AUCprobs <-  HandTill2001::auc(multcap(response=as.factor(y),predicted=probs))

errs <- sum(y!=ys)/length(y)
errp <- sum(y!=yp)/length(y)

briers <- brier(scores,y)
brierp <- brier(probs,y) 

logls <- mlogloss(scores,y)
loglp <- mlogloss(probs,y) 

out <- cbind(c(AUCscores,AUCprobs),c(errs,errp),c(briers,brierp),c(logls,loglp))
colnames(out) <- c("auc","misclassification","brier score","logloss")
rownames(out) <- c("raw scores","calibrated scores")

y.true <- y
y.score <- ys
y.calibrated <- yp
```

## Consensus clustering 
```{r}
require(pheatmap)
cons_cluster <- readRDS(file = "./data/raw_data/220609_NDD.consClust_DMP.Rds")
cons_cluster <- cons_cluster[[5]]
```


```{r}
plotCCheatmap = function(ccObject, metaData, previousCC = NULL, colorList){
  ccHmap = ccObject$consensusMatrix
  ccDist = ccObject$consensusTree
  ccDist_rev = ccDist
  ccDist_rev$order = rev(ccDist_rev$order)
  
  attr(ccHmap, "dimnames") = list(names(ccObject$consensusClass),
                                names(ccObject$consensusClass))
  
  metaData = as.data.frame(metaData)
  if(all.equal(rownames(metaData), as.character(seq_len(nrow(metaData)))))
    rownames(metaData) = metaData$Sample_Name
  metaData = metaData %>% 
    dplyr::select(-Sample_Name) 
  if(!is.null(previousCC))
    metaData[[previousCC]][is.na(metaData[[previousCC]])] = "new_sample"
  
  pheatmap(ccHmap,
           scale = "none",
           color = colorRampPalette(c("white", "blue"))(100),
           cluster_rows = ccDist_rev,
           cluster_cols = ccDist,
           show_rownames = F,
           show_colnames = F,
           cellheight = 2,
           cellwidth = 2,
           annotation_col = metaData,
           annotation_colors = colorList)
}
```

```{r, fig.height = 10, fig.width = 14}
require(dplyr)
meta_data <- read.table(file = "./data/meta_data/training_meta_data_new.txt", sep = "\t", header = T)

scores_plot <- scores[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(scores_plot)[apply(scores_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(scores_plot)[apply(scores_plot, 1, which.max)]")] <- "rf"

#now for the calibrated scores
probs_plot <- probs[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(probs_plot)[apply(probs_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(probs_plot)[apply(probs_plot, 1, which.max)]")] <- "rf_calibrated"

#meta_data['groups'] <- !is.na(meta_data$Cell_type_groups)
#meta_data$groups[meta_data$groups == TRUE] <- "UIU"
#meta_data$groups[grep("^aP", meta_data$Sample_Name)] <- "aP"
#meta_data$groups[meta_data$groups==FALSE] <- "Chan"
#
#meta_data["corretly_classified"] <- meta_data$CC_Epi_newLRO==meta_data$rf_calibrated

max_score <- apply(probs[meta_data$Sample_Name, ], 1, max)
meta_data <- cbind(meta_data, max_score)


plotCCheatmap(cons_cluster, 
              metaData = meta_data %>% 
                dplyr::select(Sample_Name, 
                              DAXX_ATRX, 
                              MEN1, 
                              CC_Epi_newLRO,
                              rf,
                              rf_calibrated, 
                              max_score),

              colorList = list(DAXX_ATRX = c(wt = "white", mut = "goldenrod4"),
                               MEN1 = c(wt = "white", mut = "goldenrod1"),
                               ConsClus_k4_UB_UCL_ICGC_Chan = c(Alpha_like = "dodgerblue4",
                                                                Beta_like = "green",
                                                                Intermediate_ADM = "lightskyblue1",
                                                                Intermediate_WT = "darksalmon",
                                                                new_sample = "yellow"),
                               CC_Epi_newLRO = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               rf = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"), 
                               rf_calibrated = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               max_score = colorRampPalette(c("white", "blue"))(100)))



```

The consensus cluster was used to "create" a response to train the random forest algorithm. The image above shows were the samples were classified according to the consensus cluster, the random forest and the random forest after calibration. 



```{r}
require(kableExtra)
table(meta_data$groups,meta_data$corretly_classified) %>% kbl %>% kable_styling()
```

Table shows the number of correctly classified and wrongly classified of the original cohorts. 

## Confusion matrix
Questions: 

* Is the consensus clutering and good response ? 
* Should the wrongly classified samples be looked again clinically ? reanalyzed ? 
* ... 

After training of the classifier with the top 10'000 probes, the calibrated random forest algorithm classified the samples as follows. 
```{r}
require(pheatmap)
load(file = "./results/rf.pred.2000.RData")

m <-table(y.true, y.calibrated)
m <- m/apply(m, 1, sum)

pheatmap(m,
         display_numbers = table(y.true, y.calibrated),
         angle_col = 45,
         treeheight_row = 0,
         treeheight_col = 0,
         cluster_rows = F,
         cluster_cols = F,
         color=colorRampPalette(c("white", "red"))(100))
```

Shows confusion matrix as heatmap. Numbers in the cells indicates the number of samples counted in that cell. The color indicates the percentage of samples correctly identified in that class. 

Should be read for right to left: there are 6 Intermidiate_ADM that were classified in the Alpha_like class. 

Looking at the 


```{r}
require(pROC)
require(ggplot2)
roc_resp <- as.numeric(y.true!=y.calibrated)
predictor_roc <- apply(probs, 1, max)
roc_to_plot <- roc(response=roc_resp, predictor=predictor_roc, ci=TRUE, of="se")
```
```{r}
roc_with_ci <- function(obj) {
  ciobj <- obj$ci
  dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                       lower = ciobj[, 1],
                       upper = ciobj[, 3])
  
  ggroc(obj) +
    theme_minimal() +
    geom_abline(
      slope = 1,
      intercept = 1,
      linetype = "dashed",
      alpha = 0.7,
      color = "black"
    ) + coord_equal() +
    geom_ribbon(
      data = dat.ci,
      aes(x = x, ymin = lower, ymax = upper),
      fill = "steelblue",
      alpha = 0.3
    ) + ggtitle(paste0("AUC : ", obj$auc))
} 

roc_with_ci(roc_to_plot)
```


```{r}
require(kableExtra)
out %>% kbl %>% kable_styling()
```





```{r}
require(ggplot2)
require(ggridges)
raw_score <- as.data.frame(scores)
raw_score["predicted"] <- as.vector(y.score)
raw_score["CC_Epi_newLRO"] <- as.vector(y)
raw_score <- raw_score[raw_score$predicted == raw_score$CC_Epi_newLRO, ]

yl <- c()
for (i in 1:nrow(raw_score)) {
  x <- raw_score[i, 'predicted']
  yl <- append(yl, raw_score[i, x])
}
raw_score["max"] <- yl
rm(yl, x)

# plot the raw scores
raw <- ggplot(raw_score, aes(x = max, y = CC_Epi_newLRO, fill = CC_Epi_newLRO)) +
  ggtitle("Raw scores for correctly classified cases") +
  xlim(0,1.2) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

```{r}
require(ggplot2)
require(ggridges)
calibrated_score <- as.data.frame(probs)
calibrated_score["predicted"] <- as.vector(y.calibrated)
calibrated_score["CC_Epi_newLRO"] <- as.vector(y)
calibrated_score <- calibrated_score[calibrated_score$predicted == calibrated_score$CC_Epi_newLRO, ]

yl <- c()
for (i in 1:nrow(calibrated_score)) {
  x <- calibrated_score[i, 'predicted']
  yl <- append(yl, calibrated_score[i, x])
}
calibrated_score["max"] <- yl
rm(yl, x)

# plot the raw scores
calibrated <- ggplot(calibrated_score, aes(x = max, y = CC_Epi_newLRO, fill = CC_Epi_newLRO)) +
  xlim(0,1.2) +
  ggtitle("Calibrated scores for correctly classified cases") +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```



```{r, fig.width=10}
cowplot::plot_grid(
  raw, 
  calibrated,
  ncol = 1)

```


# Probes  in DiDomenico paper
```{r}
require(readxl)
alpha_vs_beta <-  read_xlsx(path="./data/raw_data/DiDomenico_2020_T6_alpha_like_v_beta_like.xlsx",
                           skip = 2)
alpha_vs_beta <- alpha_vs_beta[,1]
colnames(alpha_vs_beta) <- "probes"

alpha_vs_intermidiate <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T7_alpha_like_v_intermediate.xlsx",
                                   skip = 2)
alpha_vs_intermidiate <- alpha_vs_intermidiate[,1]
colnames(alpha_vs_intermidiate) <- "probes"

intermidiate_vs_beta <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T8_intermediate_v_beta_like.xlsx",
                                  skip = 2)
intermidiate_vs_beta <- intermidiate_vs_beta[,1]
colnames(intermidiate_vs_beta) <- "probes"
```

```{r}
require(randomForest)
load(file = "./results/rf.pred.2000.RData")
require(dplyr)
probes <- as.data.frame(rownames(importance(rf.pred, type=1)))
colnames(probes) <- "probes"
intersect(alpha_vs_beta, probes)
intersect(alpha_vs_intermidiate, probes)
intersect(intermidiate_vs_beta, probes)

```


```{r}
require(randomForest)
load(file = "./results/rf.pred.2000.RData")
probes <- importance(rf.pred, type = 1)
or <- order(probes,decreasing=T)
probes <- probes[order(probes, decreasing = T)]
plot(x=c(1:length(probes)),
     #log="x",
     probes,
     cex=0.3,
     ylab="Mean decrease in accuracy",
     xlab="Probes ranked")
abline(v=c(1000, 2000), col=c("blue", "red"), lty=c(2,2))
abline(h=0)
```


# Join Intermediate_ADM_WT and Intermediate_WT into one class
```{r}
rm(list = ls())
load(file = "./results/rf.pred.4_classes.RData")
probes <- importance(rf.pred, type = 1)
or <- order(probes,decreasing=T)
probes <- probes[order(probes, decreasing = T)]
plot(x=c(1:length(probes)),
     #log="x",
     probes,
     cex=0.3,
     ylab="Mean decrease in accuracy",
     xlab="Probes ranked")
abline(v=c(1000, 2000), col=c("blue", "red"), lty=c(2,2))
abline(h=0)
```


```{r}
require(randomForest)
if(!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/meth_combat_beta.Rds"))

meta_data <- read.table("./data/meta_data/training_meta_data_new.txt", sep = "\t", header = T)
load(file = "./CV_four_classes/nfolds.RData")
y <- meta_data$Four_classes

plot(NA, ylim=c(0,1), xlim=c(1,5000),log="x", xlab="n.probes", ylab="error.cv",
     main="Cross validation on top 1'000 probes")
for (i in 1:length(nfolds)){
  samp <- nfolds[[i]][[1]][[1]][["train"]]
  cv <- rfcv(trainx = t(meth_combat_beta[or, samp]), 
       trainy = as.factor(meta_data$Four_classes[samp]), 
       cv.fold = 3,
       ntree =500)
  with(cv, lines(n.var, error.cv, type="o", lwd=2, col=i))
}
```

## Consensus clustering 
```{r}
rm(list=ls())
library(pROC)
library(HandTill2001)
source(file.path("scripts", "R","multi_brier.R"))
source(file.path("scripts", "R","multi_logloss.R"))

load(file.path("results","CVresults_four_classes.RData"))

AUCscores <- HandTill2001::auc(multcap(response=as.factor(y),predicted=scores))
AUCprobs <-  HandTill2001::auc(multcap(response=as.factor(y),predicted=probs))

errs <- sum(y!=ys)/length(y)
errp <- sum(y!=yp)/length(y)

briers <- brier(scores,y)
brierp <- brier(probs,y) 

logls <- mlogloss(scores,y)
loglp <- mlogloss(probs,y) 

out <- cbind(c(AUCscores,AUCprobs),c(errs,errp),c(briers,brierp),c(logls,loglp))
colnames(out) <- c("auc","misclassification","brier score","logloss")
rownames(out) <- c("raw scores","calibrated scores")

y.true <- y
y.score <- ys
y.calibrated <- yp
```

```{r}
require(pheatmap)
cons_cluster <- readRDS(file = "./data/raw_data/220609_NDD.consClust_DMP.Rds")
cons_cluster <- cons_cluster[[5]]
```

```{r}
plotCCheatmap = function(ccObject, metaData, previousCC = NULL, colorList){
  ccHmap = ccObject$consensusMatrix
  ccDist = ccObject$consensusTree
  ccDist_rev = ccDist
  ccDist_rev$order = rev(ccDist_rev$order)
  
  attr(ccHmap, "dimnames") = list(names(ccObject$consensusClass),
                                names(ccObject$consensusClass))
  
  metaData = as.data.frame(metaData)
  if(all.equal(rownames(metaData), as.character(seq_len(nrow(metaData)))))
    rownames(metaData) = metaData$Sample_Name
  metaData = metaData %>% 
    dplyr::select(-Sample_Name) 
  if(!is.null(previousCC))
    metaData[[previousCC]][is.na(metaData[[previousCC]])] = "new_sample"
  
  pheatmap(ccHmap,
           scale = "none",
           color = colorRampPalette(c("white", "blue"))(100),
           cluster_rows = ccDist_rev,
           cluster_cols = ccDist,
           show_rownames = F,
           show_colnames = F,
           cellheight = 2,
           cellwidth = 2,
           annotation_col = metaData,
           annotation_colors = colorList)
}
```

```{r, fig.height = 10, fig.width = 14}
require(dplyr)
meta_data <- read.table(file = "./data/meta_data/training_meta_data_new.txt", sep = "\t", header = T)

scores_plot <- scores[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(scores_plot)[apply(scores_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(scores_plot)[apply(scores_plot, 1, which.max)]")] <- "rf"

#now for the calibrated scores
probs_plot <- probs[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(probs_plot)[apply(probs_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(probs_plot)[apply(probs_plot, 1, which.max)]")] <- "rf_calibrated"

#meta_data['groups'] <- !is.na(meta_data$Cell_type_groups)
#meta_data$groups[meta_data$groups == TRUE] <- "UIU"
#meta_data$groups[grep("^aP", meta_data$Sample_Name)] <- "aP"
#meta_data$groups[meta_data$groups==FALSE] <- "Chan"
#meta_data["corretly_classified"] <- meta_data$Four_classes==meta_data$rf_calibrated

max_score <- apply(probs[meta_data$Sample_Name, ], 1, max)
meta_data <- cbind(meta_data, max_score)


plotCCheatmap(cons_cluster, 
              metaData = meta_data %>% 
                dplyr::select(Sample_Name, 
                              DAXX_ATRX, 
                              MEN1,
                              rf,
                              rf_calibrated,
                              Four_classes,
                              max_score),

              colorList = list(DAXX_ATRX = c(wt = "white", mut = "goldenrod4"),
                               MEN1 = c(wt = "white", mut = "goldenrod1"),
                               ConsClus_k4_UB_UCL_ICGC_Chan = c(Alpha_like = "dodgerblue4",
                                                                Beta_like = "green",
                                                                Intermediate_ADM = "lightskyblue1",
                                                                Intermediate_WT = "darksalmon",
                                                                new_sample = "yellow"),
                               rf = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate = "hotpink3"), 
                               rf_calibrated = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate = "hotpink3"),
                               Four_classes = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate = "hotpink3"),
                               max_score = colorRampPalette(c("white", "blue"))(100)))



```

```{r}
require(pheatmap)
load(file = "./results/rf.pred.4_classes.RData")

m <-table(y.true, y.calibrated)
m <- m/apply(m, 1, sum)

pheatmap(m,
         display_numbers = table(y.true, y.calibrated),
         angle_col = 45,
         treeheight_row = 0,
         treeheight_col = 0,
         cluster_rows = F,
         cluster_cols = F,
         color=colorRampPalette(c("white", "red"))(100))
```



```{r}
require(kableExtra)
out %>% kbl %>% kable_styling()
```


```{r}
require(pROC)
require(ggplot2)
roc_resp <- as.numeric(y.true!=y.calibrated)
predictor_roc <- apply(probs, 1, max)
roc_to_plot <- roc(response=roc_resp, predictor=predictor_roc, ci=TRUE, of="se")
```

```{r}
roc_with_ci <- function(obj) {
  ciobj <- obj$ci
  dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                       lower = ciobj[, 1],
                       upper = ciobj[, 3])
  
  ggroc(obj) +
    theme_minimal() +
    geom_abline(
      slope = 1,
      intercept = 1,
      linetype = "dashed",
      alpha = 0.7,
      color = "black"
    ) + coord_equal() +
    geom_ribbon(
      data = dat.ci,
      aes(x = x, ymin = lower, ymax = upper),
      fill = "steelblue",
      alpha = 0.3
    ) + ggtitle(paste0("AUC : ", obj$auc))
} 

roc_with_ci(roc_to_plot)
```

```{r}
require(ggplot2)
require(ggridges)
raw_score <- as.data.frame(scores)
raw_score["predicted"] <- as.vector(y.score)
raw_score["Four_classes"] <- as.vector(y)
raw_score <- raw_score[raw_score$predicted == raw_score$Four_classes, ]

yl <- c()
for (i in 1:nrow(raw_score)) {
  x <- raw_score[i, 'predicted']
  yl <- append(yl, raw_score[i, x])
}
raw_score["max"] <- yl
rm(yl, x)

# plot the raw scores
raw <- ggplot(raw_score, aes(x = max, y = Four_classes, fill = Four_classes)) +
  ggtitle("Raw scores for correctly classified cases") +
  xlim(0,1.2) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

```{r}
require(ggplot2)
require(ggridges)
calibrated_score <- as.data.frame(probs)
calibrated_score["predicted"] <- as.vector(y.calibrated)
calibrated_score["Four_classes"] <- as.vector(y)
calibrated_score <- calibrated_score[calibrated_score$predicted == calibrated_score$Four_classes, ]

yl <- c()
for (i in 1:nrow(calibrated_score)) {
  x <- calibrated_score[i, 'predicted']
  yl <- append(yl, calibrated_score[i, x])
}
calibrated_score["max"] <- yl
rm(yl, x)

# plot the raw scores
calibrated <- ggplot(calibrated_score, aes(x = max, y = Four_classes, fill = Four_classes)) +
  xlim(0,1.2) +
  ggtitle("Calibrated scores for correctly classified cases") +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```



```{r, fig.width=10}
cowplot::plot_grid(
  raw, 
  calibrated,
  ncol = 1)
```


![image](./results/fine_tuning.png)