{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pyreadr\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meth_combat_beta = pyreadr.read_r(\"./data/results/meth_combat_beta.Rds\")\n",
    "betas = meth_combat_beta[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"./data/meta_data/training_meta_data.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20000, \n",
    "                               criterion=\"gini\", \n",
    "                               n_jobs=-1, \n",
    "                               max_features=\"sqrt\",\n",
    "                               verbose=1, random_state=1234, \n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=meta_data.CC_Epi_newLRO\n",
    "betas = betas.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done 3986 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 4936 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 5986 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 7136 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8386 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 9736 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 11186 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done 12736 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 14386 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=-1)]: Done 16136 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 17986 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 19936 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done 20000 out of 20000 | elapsed:   23.2s finished\n"
     ]
    }
   ],
   "source": [
    "rf_pred = forest.fit(betas, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cg00000029', 'cg00000109', 'cg00000165', 'cg00000236', 'cg00000289',\n",
       "       'cg00000292', 'cg00000321', 'cg00000363', 'cg00000622', 'cg00000658',\n",
       "       ...\n",
       "       'cg27665715', 'cg27665754', 'cg27665767', 'cg27665769', 'cg27665829',\n",
       "       'cg27665860', 'cg27665925', 'cg27665985', 'cg27666046', 'cg27666123'],\n",
       "      dtype='object', length=384412)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = pd.DataFrame(index=betas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft[\"values\"] = rf_pred.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = soft.sort_values(by=\"values\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = x[0:10000].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg02890194</th>\n",
       "      <th>cg05176991</th>\n",
       "      <th>cg05322332</th>\n",
       "      <th>cg07697770</th>\n",
       "      <th>cg23168603</th>\n",
       "      <th>cg22396555</th>\n",
       "      <th>cg16300300</th>\n",
       "      <th>cg00719767</th>\n",
       "      <th>cg01771019</th>\n",
       "      <th>cg16283362</th>\n",
       "      <th>...</th>\n",
       "      <th>cg07490447</th>\n",
       "      <th>cg08509991</th>\n",
       "      <th>cg03252605</th>\n",
       "      <th>cg17487741</th>\n",
       "      <th>cg08637403</th>\n",
       "      <th>cg25372195</th>\n",
       "      <th>cg20050093</th>\n",
       "      <th>cg00622010</th>\n",
       "      <th>cg10462187</th>\n",
       "      <th>cg12446613</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ICGC_0425</th>\n",
       "      <td>0.707931</td>\n",
       "      <td>0.050152</td>\n",
       "      <td>0.588198</td>\n",
       "      <td>0.707620</td>\n",
       "      <td>0.970341</td>\n",
       "      <td>0.902753</td>\n",
       "      <td>0.725825</td>\n",
       "      <td>0.608920</td>\n",
       "      <td>0.596341</td>\n",
       "      <td>0.776469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751414</td>\n",
       "      <td>0.892828</td>\n",
       "      <td>0.678470</td>\n",
       "      <td>0.761772</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.864138</td>\n",
       "      <td>0.645777</td>\n",
       "      <td>0.670399</td>\n",
       "      <td>0.249808</td>\n",
       "      <td>0.758875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGC_0427</th>\n",
       "      <td>0.752506</td>\n",
       "      <td>0.059642</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.707943</td>\n",
       "      <td>0.618467</td>\n",
       "      <td>0.664322</td>\n",
       "      <td>0.799501</td>\n",
       "      <td>0.657538</td>\n",
       "      <td>0.823040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923014</td>\n",
       "      <td>0.736409</td>\n",
       "      <td>0.407928</td>\n",
       "      <td>0.666415</td>\n",
       "      <td>0.639941</td>\n",
       "      <td>0.932495</td>\n",
       "      <td>0.556786</td>\n",
       "      <td>0.455803</td>\n",
       "      <td>0.047634</td>\n",
       "      <td>0.189470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGC_0428</th>\n",
       "      <td>0.644527</td>\n",
       "      <td>0.668769</td>\n",
       "      <td>0.115122</td>\n",
       "      <td>0.169104</td>\n",
       "      <td>0.552051</td>\n",
       "      <td>0.588767</td>\n",
       "      <td>0.706693</td>\n",
       "      <td>0.431893</td>\n",
       "      <td>0.499965</td>\n",
       "      <td>0.661408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.761163</td>\n",
       "      <td>0.356976</td>\n",
       "      <td>0.255015</td>\n",
       "      <td>0.164592</td>\n",
       "      <td>0.711117</td>\n",
       "      <td>0.715966</td>\n",
       "      <td>0.950807</td>\n",
       "      <td>0.239885</td>\n",
       "      <td>0.077444</td>\n",
       "      <td>0.194576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGC_0432</th>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.199715</td>\n",
       "      <td>0.135574</td>\n",
       "      <td>0.286222</td>\n",
       "      <td>0.426559</td>\n",
       "      <td>0.861836</td>\n",
       "      <td>0.765026</td>\n",
       "      <td>0.274755</td>\n",
       "      <td>0.567295</td>\n",
       "      <td>0.645285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763028</td>\n",
       "      <td>0.875288</td>\n",
       "      <td>0.733044</td>\n",
       "      <td>0.669588</td>\n",
       "      <td>0.897555</td>\n",
       "      <td>0.571859</td>\n",
       "      <td>0.966887</td>\n",
       "      <td>0.393842</td>\n",
       "      <td>0.097071</td>\n",
       "      <td>0.205413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICGC_0434</th>\n",
       "      <td>0.118832</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>0.100309</td>\n",
       "      <td>0.614697</td>\n",
       "      <td>0.873519</td>\n",
       "      <td>0.498516</td>\n",
       "      <td>0.253709</td>\n",
       "      <td>0.636365</td>\n",
       "      <td>0.366998</td>\n",
       "      <td>0.276645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857941</td>\n",
       "      <td>0.638625</td>\n",
       "      <td>0.202660</td>\n",
       "      <td>0.908087</td>\n",
       "      <td>0.350176</td>\n",
       "      <td>0.226508</td>\n",
       "      <td>0.964344</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.115932</td>\n",
       "      <td>0.079298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aP431</th>\n",
       "      <td>0.793836</td>\n",
       "      <td>0.091032</td>\n",
       "      <td>0.324460</td>\n",
       "      <td>0.685826</td>\n",
       "      <td>0.863458</td>\n",
       "      <td>0.710759</td>\n",
       "      <td>0.762705</td>\n",
       "      <td>0.634039</td>\n",
       "      <td>0.571590</td>\n",
       "      <td>0.776232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866071</td>\n",
       "      <td>0.893721</td>\n",
       "      <td>0.570555</td>\n",
       "      <td>0.668797</td>\n",
       "      <td>0.855745</td>\n",
       "      <td>0.869021</td>\n",
       "      <td>0.710043</td>\n",
       "      <td>0.626054</td>\n",
       "      <td>0.403382</td>\n",
       "      <td>0.632648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aP464</th>\n",
       "      <td>0.819690</td>\n",
       "      <td>0.261591</td>\n",
       "      <td>0.256036</td>\n",
       "      <td>0.509474</td>\n",
       "      <td>0.696043</td>\n",
       "      <td>0.870535</td>\n",
       "      <td>0.712599</td>\n",
       "      <td>0.426922</td>\n",
       "      <td>0.643598</td>\n",
       "      <td>0.800339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799059</td>\n",
       "      <td>0.899169</td>\n",
       "      <td>0.474552</td>\n",
       "      <td>0.636920</td>\n",
       "      <td>0.837115</td>\n",
       "      <td>0.877924</td>\n",
       "      <td>0.356656</td>\n",
       "      <td>0.476818</td>\n",
       "      <td>0.123521</td>\n",
       "      <td>0.545930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aP318</th>\n",
       "      <td>0.601734</td>\n",
       "      <td>0.086884</td>\n",
       "      <td>0.205145</td>\n",
       "      <td>0.157684</td>\n",
       "      <td>0.813984</td>\n",
       "      <td>0.738758</td>\n",
       "      <td>0.678593</td>\n",
       "      <td>0.307036</td>\n",
       "      <td>0.401137</td>\n",
       "      <td>0.806070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847946</td>\n",
       "      <td>0.875695</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.551393</td>\n",
       "      <td>0.797265</td>\n",
       "      <td>0.689416</td>\n",
       "      <td>0.801792</td>\n",
       "      <td>0.462496</td>\n",
       "      <td>0.398251</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aP328</th>\n",
       "      <td>0.605725</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>0.400192</td>\n",
       "      <td>0.637374</td>\n",
       "      <td>0.687790</td>\n",
       "      <td>0.694266</td>\n",
       "      <td>0.704974</td>\n",
       "      <td>0.655293</td>\n",
       "      <td>0.670175</td>\n",
       "      <td>0.189549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762256</td>\n",
       "      <td>0.514441</td>\n",
       "      <td>0.168985</td>\n",
       "      <td>0.271743</td>\n",
       "      <td>0.763566</td>\n",
       "      <td>0.799334</td>\n",
       "      <td>0.828666</td>\n",
       "      <td>0.229579</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.472302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aP348</th>\n",
       "      <td>0.685828</td>\n",
       "      <td>0.308416</td>\n",
       "      <td>0.216303</td>\n",
       "      <td>0.542347</td>\n",
       "      <td>0.887234</td>\n",
       "      <td>0.732806</td>\n",
       "      <td>0.502910</td>\n",
       "      <td>0.700648</td>\n",
       "      <td>0.460377</td>\n",
       "      <td>0.835843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755234</td>\n",
       "      <td>0.892628</td>\n",
       "      <td>0.661154</td>\n",
       "      <td>0.812021</td>\n",
       "      <td>0.843199</td>\n",
       "      <td>0.856295</td>\n",
       "      <td>0.833600</td>\n",
       "      <td>0.623098</td>\n",
       "      <td>0.090111</td>\n",
       "      <td>0.430198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows Ã— 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cg02890194  cg05176991  cg05322332  cg07697770  cg23168603  \\\n",
       "ICGC_0425    0.707931    0.050152    0.588198    0.707620    0.970341   \n",
       "ICGC_0427    0.752506    0.059642    0.081624    0.111592    0.707943   \n",
       "ICGC_0428    0.644527    0.668769    0.115122    0.169104    0.552051   \n",
       "ICGC_0432    0.736120    0.199715    0.135574    0.286222    0.426559   \n",
       "ICGC_0434    0.118832    0.019496    0.100309    0.614697    0.873519   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "aP431        0.793836    0.091032    0.324460    0.685826    0.863458   \n",
       "aP464        0.819690    0.261591    0.256036    0.509474    0.696043   \n",
       "aP318        0.601734    0.086884    0.205145    0.157684    0.813984   \n",
       "aP328        0.605725    0.053327    0.400192    0.637374    0.687790   \n",
       "aP348        0.685828    0.308416    0.216303    0.542347    0.887234   \n",
       "\n",
       "           cg22396555  cg16300300  cg00719767  cg01771019  cg16283362  ...  \\\n",
       "ICGC_0425    0.902753    0.725825    0.608920    0.596341    0.776469  ...   \n",
       "ICGC_0427    0.618467    0.664322    0.799501    0.657538    0.823040  ...   \n",
       "ICGC_0428    0.588767    0.706693    0.431893    0.499965    0.661408  ...   \n",
       "ICGC_0432    0.861836    0.765026    0.274755    0.567295    0.645285  ...   \n",
       "ICGC_0434    0.498516    0.253709    0.636365    0.366998    0.276645  ...   \n",
       "...               ...         ...         ...         ...         ...  ...   \n",
       "aP431        0.710759    0.762705    0.634039    0.571590    0.776232  ...   \n",
       "aP464        0.870535    0.712599    0.426922    0.643598    0.800339  ...   \n",
       "aP318        0.738758    0.678593    0.307036    0.401137    0.806070  ...   \n",
       "aP328        0.694266    0.704974    0.655293    0.670175    0.189549  ...   \n",
       "aP348        0.732806    0.502910    0.700648    0.460377    0.835843  ...   \n",
       "\n",
       "           cg07490447  cg08509991  cg03252605  cg17487741  cg08637403  \\\n",
       "ICGC_0425    0.751414    0.892828    0.678470    0.761772    0.914179   \n",
       "ICGC_0427    0.923014    0.736409    0.407928    0.666415    0.639941   \n",
       "ICGC_0428    0.761163    0.356976    0.255015    0.164592    0.711117   \n",
       "ICGC_0432    0.763028    0.875288    0.733044    0.669588    0.897555   \n",
       "ICGC_0434    0.857941    0.638625    0.202660    0.908087    0.350176   \n",
       "...               ...         ...         ...         ...         ...   \n",
       "aP431        0.866071    0.893721    0.570555    0.668797    0.855745   \n",
       "aP464        0.799059    0.899169    0.474552    0.636920    0.837115   \n",
       "aP318        0.847946    0.875695    0.691919    0.551393    0.797265   \n",
       "aP328        0.762256    0.514441    0.168985    0.271743    0.763566   \n",
       "aP348        0.755234    0.892628    0.661154    0.812021    0.843199   \n",
       "\n",
       "           cg25372195  cg20050093  cg00622010  cg10462187  cg12446613  \n",
       "ICGC_0425    0.864138    0.645777    0.670399    0.249808    0.758875  \n",
       "ICGC_0427    0.932495    0.556786    0.455803    0.047634    0.189470  \n",
       "ICGC_0428    0.715966    0.950807    0.239885    0.077444    0.194576  \n",
       "ICGC_0432    0.571859    0.966887    0.393842    0.097071    0.205413  \n",
       "ICGC_0434    0.226508    0.964344    0.315789    0.115932    0.079298  \n",
       "...               ...         ...         ...         ...         ...  \n",
       "aP431        0.869021    0.710043    0.626054    0.403382    0.632648  \n",
       "aP464        0.877924    0.356656    0.476818    0.123521    0.545930  \n",
       "aP318        0.689416    0.801792    0.462496    0.398251    0.449300  \n",
       "aP328        0.799334    0.828666    0.229579    0.040330    0.472302  \n",
       "aP348        0.856295    0.833600    0.623098    0.090111    0.430198  \n",
       "\n",
       "[155 rows x 10000 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas[probes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 736 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1186 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1736 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 2386 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 3136 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3986 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 4936 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 5986 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 7136 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 8386 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 9736 tasks      | elapsed:   18.4s\n",
      "[Parallel(n_jobs=-1)]: Done 11186 tasks      | elapsed:   21.1s\n",
      "[Parallel(n_jobs=-1)]: Done 12736 tasks      | elapsed:   24.0s\n",
      "[Parallel(n_jobs=-1)]: Done 14386 tasks      | elapsed:   27.1s\n",
      "[Parallel(n_jobs=-1)]: Done 16136 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done 17986 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 19936 tasks      | elapsed:   37.5s\n",
      "[Parallel(n_jobs=-1)]: Done 20000 out of 20000 | elapsed:   37.5s finished\n"
     ]
    }
   ],
   "source": [
    "final_forest = forest.fit(betas[probes], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ClassifierMixin.score of RandomForestClassifier(n_estimators=20000, n_jobs=-1, verbose=1)>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_forest.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69\n",
      "  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51 104 105\n",
      " 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 153 154] TEST: [ 52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69\n",
      "  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n",
      "  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103] TEST: [104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 136 tasks      | elapsed:   28.6s\n",
      "[Parallel(n_jobs=-1)]: Done 386 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y=meta_data.CC_Epi_newLRO\n",
    "le.fit(np.unique(y))\n",
    "y = np.array([le.transform(y)])\n",
    "y = y.ravel()\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=500, \n",
    "                             n_jobs=-1, \n",
    "                             verbose=1)\n",
    "\n",
    "cv = model_selection.KFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in cv.split(betas):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = betas.iloc[train_index,:], betas.iloc[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # For training, fit() is used\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Default metric is R2 for regression, which can be accessed by score()\n",
    "    model.score(X_test, y_test)\n",
    " \n",
    "    # For other metrics, we need the predictions of the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics.mean_squared_error(y_test, y_pred)\n",
    "    metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=32)]: Using backend ThreadingBackend with 32 concurrent workers.\n",
      "[Parallel(n_jobs=32)]: Done 136 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 386 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=32)]: Done 500 out of 500 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37795771749999996"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765296156862745\n",
      "0.37795771749999996\n"
     ]
    }
   ],
   "source": [
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(metrics.r2_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "24d63d84b1736eae41d53957d29d0577473120e6dd1afb46ca9b41986dd0c836"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
