---
title: "043_PanNET_Marco_Visani"
author: "Marco Visani"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
 
# Part 1
First load required functions 
```{r, error=FALSE, message=F, include=FALSE}
#setwd("P:/Forschung/GRP Perren_Marinoni/1. Group/2. People/2. Students/Marco_Visani")
#rm(list = ls())
library(DMRcate)
library(dplyr)
library(ChAMP)
library(minfi)
library(readr)
library(base)
library(doParallel)
source("ChAMP_functions_Lionel_Philipp_220727.R")
#source("test.R")
```

# EPIC pre-process
Read the raw data.The function readRDS allows the read R Data Format: RDS
```{r}
path_epic <- "./data/raw_data/training_EPIC.Rds"
meth_epic.rgSet <-  readRDS(path_epic)
```

```{r}
# get detection p values
#detectionP(METH.rgSet)
#getNBeads(METH.rgSet)
```


## run NOOB 
```{r message=FALSE}
meth_epic <- champ.load_extended(rgSet_object = meth_epic.rgSet, 
                           sampleSheet = meth_epic.sampleSheet,
                           method = "minfi", 
                           filterDetP = T, # include low p value probes
                           filterBeads = T, # include probes detected in few beads
                           beadCutoff = 1,
                           detPcut = 1,
                           arraytype = "EPIC", # set accorgingly
                           preproc = "Noob", 
                           dyeMethod = "single", 
                           dataToInclude = c("loadQC", "mset"),
                           force = F)
rm(meth_epic.rgSet)
```

Removing legacy probes. 
```{r}
EPIC_legacy_probes <-  read_delim("P:/Forschung/GRP Perren_Marinoni/1. Group/2. People/17. Philipp Kirchner/projects/shared_data/Illumina_methylation_arrays/MethylationEPIC Missing Legacy CpG (v1.0_B3 vs. v1.0_B2).txt.gz",
                                delim = "\t",
                                show_col_types = F)

meth_epic$mset <- meth_epic$mset[!(rownames(meth_epic$mset) %in% EPIC_legacy_probes$TargetID), ]
if(exists("EPIC.manifest.hg19"))
  rm(EPIC.manifest.hg19)
```

<<<<<<< HEAD
remove variables from ChAMP source file and add data to empty list
=======

## convert to beta values (meth / unmeth + meth)
shared_probes is the row names -> order them and then fuse both files
```{r}
row_names_epic <- sort(rownames(meth_epic$mset))
meth_epic.beta <- getBeta(meth_epic$mset, "Illumina")[row_names_epic, ]


rm(meth_epic, row_names_epic) #remove data that we don't need anymore... (to test)
```



# 450k pre-process
```{r}
path_450k <- "./data/raw_data/training_450K.Rds"
meth_450.rgSet <- readRDS(path_450k)
```

## run NOOB 
```{r, message=FALSE, include=FALSE}
meth_450 <- champ.load_extended(rgSet_object = meth_450.rgSet, 
                           sampleSheet = meth_450.sampleSheet,
                           method = "minfi", 
                           filterDetP = T, # include low p value probes
                           filterBeads = T, # include probes detected in few beads
                           beadCutoff = 1,
                           detPcut = 1,
                           arraytype = "450K", # set accorgingly
                           preproc = "Noob", 
                           dyeMethod = "single", 
                           dataToInclude = c("loadQC", "mset"),
                           force = F)
rm(meth_450.rgSet)
```

remove legacy probes from 450k array
```{r}
meth_450$mset <- meth_450$mset[!(rownames(meth_450$mset) %in% EPIC_legacy_probes$TargetID), ]
```


## convert beta values for 450k sample
```{r}
row_names_450 <- sort(rownames(meth_450$mset))
meth_450.beta <- getBeta(meth_450$mset, "Illumina")[row_names_450, ]
rm(meth_450, row_names_450)
```




# Combine 450K and EPIC beta matrices (intersection)
```{r}
row_names <- intersect(rownames(meth_epic.beta), rownames(meth_450.beta))
meth.beta <- cbind(meth_450.beta[row_names, ], meth_epic.beta[row_names,])

#remove all unuseful data to free up RAM
rm(EPIC_legacy_probes, hm450.manifest.hg19, multi.hit, probe.features, row_names, meth_450.beta, meth_epic.beta)

```



## put probes in ascending order
Done before

#Normalize dataset
```{r, message=F, include=FALSE}
n_cores <- detectCores()/2

#run function
meth.norm <- champ.norm(meth.beta, 
                       arraytype = "450K", 
                       cores = 1,
                       resultsDir = "results")
saveRDS(meth.norm, file="meth_normalized.Rds", compress = T)
if(exists("meth.beta"))
  rm(meth.beta)
#write.table(meth.norm, file = "meth_normalized.txt")
```


# Remove batch effect and look for surrogate variable
Now that we have our dataset is normalized, we ant to make sure that all the samples are in the same order than in the metadata.


```{r}
if(!exists("meth.norm"))
  meth.norm <- readRDS("./data/results/meth_normalized.Rds")
if(!exists("meta_data"))
  meta_data <- read.table("./data/meta_data/training_meta_data.txt", sep = "\t", header = T)

meth.norm <- meth.norm[, meta_data$Sample_Name]

require(sva)

meth_cb_model <- model.matrix(~ 1 , data = meta_data)

meth_combat <- ComBat(ENmix::B2M(meth.norm),
                 batch = meta_data$Slide,
                 mod = meth_cb_model,
                 BPPARAM = bpparam("SerialParam"))
```

## Are there significant surrogate variables?

```{r}
sva_model <- model.matrix(~ CC_Epi_newLRO, data = meta_data)
sva_model_zero <- model.matrix(~ 1, data = meta_data)
meth_sva <- sva::sva(dat = meth_combat, 
                            mod = sva_model,
                            mod0 = sva_model_zero, 
                            n.sv = 42) #42 found by running it without this parameter (takes a while)
```

## Do surrogate variable describe any known variable ?
```{r, fig.width = 14}
require(ggplot2)
#this part will tell if the surrogate variables are descibing other varibales of the dataset
methSV <- as.data.frame(meth_sva$sv)
colnames(methSV) <- paste0("S", colnames(methSV))
methSV <- cbind(meta_data,methSV) 

SVList = as.list(rep(NA, times = sum(grepl("SV", colnames(methSV)))))
for (sv in seq_along(SVList)){
  SVList[[sv]] = lapply(colnames(meta_data), function(var){
    if(is.numeric(methSV[[var]]))
      test_res = summary(lm(as.formula(paste0(var, "~ SV", sv)), data = methSV))$coef[2,4]
    else
      test_res = kruskal.test(as.formula(paste0(var, "~ SV", sv)), data = methSV)$p.value
    data.frame(variable = var,
               pval = test_res,
               SV = paste0("SV", sv))
  })
}

#This part plot the 
bind_rows(SVList) %>% 
  mutate(pval_bin = cut(-log10(pval), 
                        breaks = c(0, -log10(0.05), 2, 5, 10, 300), 
                        labels = c("a", "b", "c", "d", "e"), 
                        include.lowest = T)) %>% 
  ggplot(aes(x = SV, y = variable, fill = pval_bin)) + 
  geom_tile() + 
  coord_fixed() + 
  scale_fill_manual(values = c("white", "pink", "orange", "red", "darkred"),
                    limits = c("a", "b", "c", "d", "e"),
                    labels = c("p >= 0.05", "p < 0.05", "p < 0.01", 
                               expression("p < 1x" ~ 10^{-5}), 
                               expression("p < 1x" ~ 10^{-10}))) +
  theme_classic() + 
  theme(legend.title = element_blank(), 
        legend.text.align = 0, 
        legend.background = element_rect(fill = "grey90"), 
        axis.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
  labs(title = "SVs vs biological variables")
```
Good ? we have removed 


#Run PCA

most variable probes
```{r, fig.height=10}
require(fafreqs)
probevar <- rowVars(meth_combat)
probevar <- order(probevar, decreasing = T)

meth_combat_pca <- meth_combat[probevar[1:30000],meta_data$Sample_Name] #30000 is an arbitrary number --> look at paper on brain tumors (they took 32000)

meth_combat_pca <- t(meth_combat_pca)
pca_result <- prcomp(meth_combat_pca)

pca_df = pca_result$x %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample_Name")
pca_df = left_join(pca_df, 
                   meta_data,
                   by = "Sample_Name")
pca_eig = factoextra::get_eigenvalue(pca_result)



cowplot::plot_grid(
  ggplot(pca_df, aes(color=Technology, x=PC1, y=PC2))+
    geom_point(),
  
  ggplot(pca_df, aes(color=MEN1, x=PC1, y=PC2))+
    geom_point(),
  
  ggplot(pca_df, aes(color=DAXX_ATRX, x=PC1, y=PC2))+
    geom_point(),
  
  ggplot(pca_df, aes(color=Grade, x=PC1, y=PC2))+
    geom_point(),
  
  ggplot(pca_df, aes(color=MCT4_max, x=PC1, y=PC2))+
    geom_point(),
  
  ggplot(pca_df, aes(color=CC_Epi_newLRO, x=PC1, y=PC2))+
    geom_point(), ncol =2, align = "v" 
)

```


#Run t-SNE
```{r, fig.height=10}
require(Rtsne)
require(RColorBrewer)
set.seed(123) # setting a fixed seed allows reproducibility 
tsne <- Rtsne::Rtsne(meth_combat_pca, theta=0.0, PCA=F,
                     max_iter=2500) #with default perplexity of 30. Theta set to 0.0 for higher accuracy (but lower speed)
                                    # PCA and max_iter are set as in Capper et al. paper
tsne_df <- tsne$Y %>% as.data.frame() %>% rownames_to_column("Sample_Name")
tsne_df$Sample_Name <- c(rownames(meth_combat_pca))
tsne_df <- left_join(tsne_df, meta_data, by="Sample_Name")


cowplot::plot_grid(
  ggplot(tsne_df, aes(x=V1, y=V2, color=Technology))+
  xlab("tSNE1") + ylab("tSNE2")+
    geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=CC_Epi_newLRO))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MEN1))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=DAXX_ATRX))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=Grade))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MCT4_max))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), ncol =2, align = "v"
)
```

```{r}
#meth_combat_beta <- ENmix::M2B(meth_combat)
#saveRDS(meth_combat_beta, file="./data/results/20221017_meth_combat_beta.Rds")

if (!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/20221017_meth_combat_beta.Rds"))
if (!exists("meta_data"))
  meta_data <- read.table("./data/meta_data/training_meta_data.txt", sep = "\t", header = T)
```



```{r}
require(randomForest)
require(doParallel)

forest.classifier <- function(x, response, i=NA){
  require(randomForest)
  forest <- randomForest(x=t(x),
                            y=response,
                            mtry = floor(sqrt(nrow(x))),
                            ntree = 100,
                            sampsize = rep(10,5), 
                            importance = T
                            )
  
  i <- matrix(data = NA, nrow = nrow(x), ncol = 2)
  row.names(i) <- rownames(x)
  i[,1] <- importance(forest, type=1) #mean decrease in accuracy
  i[,2] <- importance(forest, type=2) #mean decrease in Gini
  
  return(i)
}

set.seed(1234)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
final_forest <- foreach(i=1:100) %dopar% forest.classifier(x=meth_combat_beta,
                                                         response=as.factor(meta_data$CC_Epi_newLRO),
                                                         i)
stopCluster(cl)

gini <- matrix(data = NA, nrow = nrow(meth_combat_beta), ncol = 100)
accuracy <- matrix(data = NA, nrow = nrow(meth_combat_beta), ncol = 100)
row.names(gini) <- rownames(meth_combat_beta);
row.names(accuracy) <- rownames(meth_combat_beta)

for (i in 1:length(final_forest)) {
  ls <- unlist(final_forest[[i]])
  ls <- ls[sort(rownames(meth_combat_beta)),]
  accuracy[,i] <- ls[,1]
  gini[,i] <- ls[,2]
}

```

#test
```{r}
require(randomForest)
require(doParallel)
forest.classifier.test <- function(x, response, i=NA, ntrees){
  require(randomForest)
  forest <- randomForest(x=t(x),
                            y=response,
                            mtry = floor(sqrt(nrow(x))),
                            ntree = ntrees,
                            sampsize = rep(min(table(response)),length(table(response))), 
                            importance = T
                            )
  
  i <- forest
  return(i)
}

no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
final_forest <- foreach(i=1:100) %dopar% forest.classifier.test(x=meth_combat_beta,
                                                         response=as.factor(meta_data$CC_Epi_newLRO),
                                                         i,
                                                         ntrees = 100)
stopCluster(cl)
test <- do.call(randomForest::combine, final_forest)
test <- importance(test, type = 1)
or <- order(test,decreasing=T)
betasy <- meth_combat_beta[or[1:10000],]

final_forest <- randomForest(t(betasy), 
                             y=as.factor(meta_data$CC_Epi_newLRO), 
                             mtry = 100,
                             ntree = 10000, 
                             sampsize = rep(10,5),
                             importance = TRUE
                             )
```




```{r}
gini_ranked <- matrix(data = NA, nrow = nrow(meth_combat_beta), ncol = 100)
accuracy_ranked <- matrix(data = NA, nrow = nrow(meth_combat_beta), ncol = 100)
row.names(gini_ranked) <- rownames(meth_combat_beta);
row.names(accuracy_ranked) <- rownames(meth_combat_beta)

for (i in 1:ncol(gini)) {
  accuracy_ranked[,i] <- rank(accuracy[,i], ties.method = "random")
  gini_ranked[,i] <- rank(gini[,i], ties.method = "random")
}

```

```{r}
require(matrixStats)
accuracy_ranked <- as.data.frame(cbind(accuracy_ranked, apply(accuracy_ranked, 1, median)))
gini_ranked <-  as.data.frame(cbind(gini_ranked, apply(gini_ranked, 1, median)))

```

```{r}
saveRDS(gini, file="./data/results/221025_gini.Rds")
saveRDS(accuracy, file="./data/results/221025_accuracy.Rds")
saveRDS(gini_ranked, file="./data/results/221025_gini_ranked.Rds")
saveRDS(accuracy_ranked, file="./data/results/221025_accuracy_ranked.Rds")
```


```{r}
if (!exists("accuracy_ranked"))
  accuracy_ranked <- as.data.frame(readRDS("./data/results/221025_accuracy_ranked.Rds"))
if (!exists("gini_ranked"))
  gini_ranked <- as.data.frame(readRDS("./data/results/221025_gini_ranked.Rds"))
if (!exists("meta_data"))
  meta_data <- read.table(file = "./data/meta_data/training_meta_data.txt", sep = "\t", header = T)
if (!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/20221017_meth_combat_beta.Rds"))
```


```{r}
require(dplyr)
accuracy_ranked <- accuracy_ranked[order(-accuracy_ranked$V101), ]
accuracy_important_probes <- rownames(accuracy_ranked)
accuracy_important_probes <- accuracy_important_probes[1:10000]

#gini_ranked <- gini_ranked[order(-gini_ranked$V101), ]
#gini_important_probes <- rownames(gini_ranked)
#gini_important_probes <- gini_important_probes[1:10000]

```

Here we found the most important probes we then chose the top 10'000 probes to create our classifier. 

## final random classifier 
```{r}
require(randomForest)
accuracy_final_model <- randomForest(x=t(meth_combat_beta[accuracy_important_probes, ]),
                             y = as.factor(meta_data$CC_Epi_newLRO), 
                             mtry = 100,
                             ntree = 10000, 
                             sampsize = rep(10,5),
                             importance = TRUE)
gini_final_model <- randomForest(x=t(meth_combat_beta[gini_important_probes, ]),
                             y = as.factor(meta_data$CC_Epi_newLRO), 
                             mtry = 100,
                             ntree = 10000, 
                             sampsize = rep(10,5),
                             importance = TRUE)


print(accuracy_final_model)
```


## cross-validation
```{r}
cross_validation <- rfcv(trainx = t(meth_combat_beta[accuracy_important_probes,]),
                         trainy = as.factor(meta_data$CC_Epi_newLRO),
                         cv.fold = 3)
with(cross_validation, plot(n.var, error.cv, log="x", type="o", lwd=2))
```


```{r}
#test with best 1'000 probes and not 10'000
accuracy_final_model_2 <- randomForest(x=t(meth_combat_beta[accuracy_important_probes[1:1000], ]),
                             y = as.factor(meta_data$CC_Epi_newLRO), 
                             mtry = sqrt(1000),
                             ntree = 10000, 
                             sampsize = rep(10,5),
                             importance = TRUE)
print(accuracy_final_model_2)
```

The last one is a test. After cross-validation, we see that we might have over fitted the training data. With 10 times less probes we see that we increase by 4% the error rate. However, this might for future data to improve the model and avoid over fitting. 
Last test is with only the 125 most variable probes :

```{r}
accuracy_final_model_3 <- randomForest(x=t(meth_combat_beta[accuracy_important_probes[1:125], ]),
                             y = as.factor(meta_data$CC_Epi_newLRO), 
                             mtry = sqrt(125),
                             ntree = 10000, 
                             sampsize = rep(10,5),
                             importance = TRUE)
print(accuracy_final_model_3)
```

Here again, we decrease in accuracy but according to the cross validation, this doesn't seem to be a big problem. --> check all this and discuss !! 


```{r}
cross_validation_2 <- rfcv(trainx = t(meth_combat_beta[accuracy_important_probes[1:1000], ]),
                         trainy = as.factor(meta_data$CC_Epi_newLRO),
                         cv.fold = 3)
with(cross_validation_2, plot(n.var, error.cv, log="x", type="o", lwd=2))
```
Last step doesn't seem correct... --> maybe with predict it works better


```{r}
cross_validation_3 <- rfcv(trainx = t(meth_combat_beta[accuracy_important_probes[1:250], ]),
                         trainy = as.factor(meta_data$CC_Epi_newLRO),
                         cv.fold = 3)
with(cross_validation_3, plot(n.var, error.cv, log="x", type="o", lwd=2))
```

# Classifier score calibration
Check with lasso and ridge regression
A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. In the paper. the used an L2 meaning that a Ridge regression should be used
```{r}
require(ggplot2)
require(ggridges)
raw_score <- as.data.frame(accuracy_final_model[["votes"]])
raw_score["predicted"] <- as.vector(accuracy_final_model[["predicted"]])
raw_score["CC_Epi_newLRO"] <- as.vector(meta_data$CC_Epi_newLRO)
raw_score <- raw_score[raw_score$predicted == raw_score$CC_Epi_newLRO, ]

y <- c()
for (i in 1:nrow(raw_score)) {
  x <- raw_score[i, 'predicted']
  y <- append(y, raw_score[i, x])
}
raw_score["max"] <- y
rm(y, x)

# plot the raw scores
ggplot(raw_score, aes(x = max, y = CC_Epi_newLRO, fill = CC_Epi_newLRO)) +
  geom_density_ridges() +
  theme_ridges() + 
  theme(legend.position = "none")
```

```{r}
votes <- as.matrix(as.data.frame(accuracy_final_model[["votes"]]))
require(glmnet)
set.seed(7)
accuracy_ridge <- glmnet(x = votes,
                         y = as.factor(meta_data$CC_Epi_newLRO),
                         family = "multinomial",
                         alpha=0,) #alpha = 0 means Ridge or L2 regression. If alpha = 1 --> Lasso regression (L1)

par(mfrow=c(1,2), cex=0.5, mar=c(3,3,4,1))
plot(accuracy_ridge, xvar="lambda", main="Ridge regression (L2)")
```


```{r}
require(glmnet)
require(tidyr)
accuracy_cv_glmnet <- cv.glmnet(x = votes,
                                y= as.factor(meta_data$CC_Epi_newLRO), 
                                family = "multinomial",
                                alpha=0,
                                type.measure = "class"
)

plot(accuracy_cv_glmnet)
coefficients <- coef(accuracy_cv_glmnet)

### Now we want to find the probes that describe the best each class
calibration_matrix <- data.frame(row.names = rownames(coefficients[[1]]))
for (i in names(coefficients)) {
  n <- as.matrix(coefficients[[i]])
  colnames(n) <- i
  calibration_matrix <- cbind(calibration_matrix, n)
  rm(n)
}


```

```{r}
makefolds <- function(y,cv.fold=3){
  n <- length(y)
  nlvl <- table(y)
  idx <- numeric(n)
  folds <- list()
  for (i in 1:length(nlvl)) {
    idx[which(y == levels(y)[i])] <- sample(rep(1:cv.fold,length = nlvl[i]))
  }
  for (i in 1:cv.fold){
    folds[[i]] <- list(train = which(idx!=i),
                       test =  which(idx==i)) 
  }  
  return(folds)
}
makenestedfolds <- function(y,cv.fold=3){
  nfolds <- list()
  folds <- makefolds(y,cv.fold)
  names(folds) <- paste0("outer",1:length(folds))
  for(k in 1:length(folds)){
    inner = makefolds(y[folds[[k]]$train],cv.fold)
    names(inner) <- paste0("inner",1:length(folds))
    for(i in 1:length(inner)){
      inner[[i]]$train <- folds[[k]]$train[inner[[i]]$train]
      inner[[i]]$test <- folds[[k]]$train[inner[[i]]$test]
    }
    nfolds[[k]] <- list(folds[k],inner) 
  }
  names(nfolds) <- paste0("outer",1:length(nfolds))
  return(nfolds)
}
```

```{r}
nfolds <- makenestedfolds(meta_data$CC_Epi_newLRO)

for(i in 1:length(nfolds)){
  scores <- list() 
  idx <- list()
  for(j in 1:length(nfolds)){
    fname <- paste0("CVfold.",i,".",j,".RData")
    load(file.path("CV",fname))
    scores[[j]] <- rf.scores
    idx[[j]] <- nfolds[[i]][[2]][[j]]$test
  }
  scores <- do.call(rbind,scores)
  idx <- unlist(idx)
  y <- anno$`methylation class:ch1`[idx]         
  
  message("fitting calbriation model fold ",i," ...",Sys.time())
  # fit multinomial logistic ridge regression model
  suppressWarnings(cv.calfit <- cv.glmnet(y=y,x=scores,family="multinomial",type.measure="mse",
                                          alpha=0,nlambda=100,lambda.min.ratio=10^-6,parallel=TRUE))
  
  fname <- paste0("CVfold.",i,".",0,".RData")
  load(file.path("CV",fname))
  
  message("calibrating raw scores fold ",i," ...",Sys.time())
  probs <- predict(cv.calfit$glmnet.fit,newx=rf.scores,type="response"
                   ,s=cv.calfit$lambda.1se)[,,1] # use lambda estimated by 10fold CVlambda
  
  
  err <- sum(colnames(probs)[apply(probs,1,which.max)] != anno$`methylation class:ch1`[nfolds[[i]][[1]][[1]]$test])/length(nfolds[[i]][[1]][[1]]$test)
  
  message("misclassification error: ",err)
  
  fname_probs <- paste0("probsCVfold.",i,".",0,".RData")
  save(probs,file=file.path("CV",fname_probs))
}
```

