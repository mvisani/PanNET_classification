---
title: "043_panet_classification_5_classes"
author: "Marco Visani"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    code_folding: hide
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,warning = F)
```


# Introduction

Data received was of RGChannelSetExtended formal class. NOOB was run for both 450 and EPIC data. In order to include as much data as possible, low p value probes and low bead count probes were also included. Legacy probes were removed (total of 997 probes). The probes were sorted by probe name. Data was converted to beta values. Taking the intersect between the probe names of EPIC and 450K technology and the then data was joined together. Beta values were then normalized using `champ.norm`function. 

Normalized beta values were converted back to M values. Data was adjusted for batch effect on the `Slide` using ComBat. Values were then converted back to beta values. 

QC was then performed (see below).  

# Quality control 
Surrogate variables, PCA and t-SNE were checked. 

## Check for surrogate variables
First we checked for surrogate variables. Some were found but on the variables that we are interested in. If some of the surrogate variables were found on the `Technology` variable for example, this should be a concern. Here, there is nothing to worry about. 
```{r}
rm(list = ls())
meta_data <- read.table("./data/meta_data/training_meta_data.txt", sep = "\t", header = T)
if(!exists("meth_combat_M"))
  meth_combat_M <- readRDS("./data/results/meth_combat_M.Rds")
if(!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/meth_combat_beta.Rds"))
if (!exists("meth_norm"))
  meth_norm <- as.data.frame(readRDS("./data/results/meth_normalized.Rds"))
```

```{r, fig.width = 14}
load(file = "./data/results/surrogate_var.RData")
require(ggplot2)
require(dplyr)
#this part will tell if the surrogate variables are descibing other varibales of the dataset
methSV <- as.data.frame(meth_sva$sv)
colnames(methSV) <- paste0("S", colnames(methSV))
methSV <- cbind(meta_data,methSV) 

SVList = as.list(rep(NA, times = sum(grepl("SV", colnames(methSV)))))
for (sv in seq_along(SVList)){
  SVList[[sv]] = lapply(colnames(meta_data), function(var){
    if(is.numeric(methSV[[var]]))
      test_res = summary(lm(as.formula(paste0("SV", sv,"~", var)), data = methSV))$coef[2,4]
    else
      test_res = kruskal.test(as.formula(paste0("SV", sv,"~", var)), data = methSV)$p.value
    data.frame(variable = var,
               pval = test_res,
               SV = paste0("SV", sv))
  })
}

#This part plot the 
bind_rows(SVList) %>% 
  mutate(pval_bin = cut(-log10(pval), 
                        breaks = c(0, -log10(0.05), 2, 5, 10, 300), 
                        labels = c("a", "b", "c", "d", "e"), 
                        include.lowest = T), SV = factor(SV, levels=paste0("SV", 1:42))) %>% 
  ggplot(aes(x = SV, y = variable, fill = pval_bin)) + 
  geom_tile() + 
  coord_fixed() + 
  scale_fill_manual(values = c("white", "pink", "orange", "red", "darkred"),
                    limits = c("a", "b", "c", "d", "e"),
                    labels = c("p >= 0.05", "p < 0.05", "p < 0.01", 
                               expression("p < 1x" ~ 10^{-5}), 
                               expression("p < 1x" ~ 10^{-10}))) +
  theme_classic() + 
  theme(legend.title = element_blank(), 
        legend.text.align = 0, 
        legend.background = element_rect(fill = "grey90"), 
        axis.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
  labs(title = "SVs vs biological variables")
```

Since we have many surrogate variables, there is nothing to worry about on the `Array` variable because the p-value is at 0.01 so there is a small percentage of chance that that SV was just assigned by chance. 

## PCA
PCA tries to preserve the global structure of data i.e when converting d-dimensional data to d'-dimensional data then it tries to map all the clusters as a whole due to which local structures might get lost. That is why we also checked with t-SNE. 
```{r, fig.height=10, fig.width=14}
require(fafreqs)
require(tibble)
#probevar <- rowVars(meth_combat_M)
probevar <- apply(meth_combat_M, 1, var)
probevar <- order(probevar, decreasing = T)

meth_combat_pca <- meth_combat_M[probevar[1:30000],meta_data$Sample_Name] #30000 is an arbitrary number --> look at paper on brain tumors (they took 32000)

meth_combat_pca <- t(meth_combat_pca)
pca_result <- prcomp(meth_combat_pca)

pca_df = pca_result$x %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample_Name")
pca_df = left_join(pca_df, 
                   meta_data,
                   by = "Sample_Name")
pca_eig = factoextra::get_eigenvalue(pca_result)



cowplot::plot_grid(
  ggplot(pca_df, aes(color=Technology, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MEN1, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=DAXX_ATRX, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=Grade, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MCT4_max, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=CC_Epi_newLRO, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  ncol =2, align = "v" 
)

```

We can't see any pattern. Maybe a small one for the Intermidiate_ADM probes classified by the `CC_Epi_newLRO` (bottom left image). 
The two first PCs only describe in total 21\% of the variance, which is quite low.

## t-SNE
```{r, fig.height=10, fig.width=14}
require(Rtsne)
require(RColorBrewer)
set.seed(123) # setting a fixed seed allows reproducibility 
tsne <- Rtsne::Rtsne(meth_combat_pca, theta=0.0, PCA=F,
                     max_iter=2500) #with default perplexity of 30. Theta set to 0.0 for higher accuracy (but lower speed)
                                    # PCA and max_iter are set as in Capper et al. paper
tsne_df <- tsne$Y %>% as.data.frame() %>% rownames_to_column("Sample_Name")
tsne_df$Sample_Name <- c(rownames(meth_combat_pca))
tsne_df <- left_join(tsne_df, meta_data, by="Sample_Name")


cowplot::plot_grid(
  ggplot(tsne_df, aes(x=V1, y=V2, color=Technology))+
  xlab("tSNE1") + ylab("tSNE2")+
    geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=CC_Epi_newLRO))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MEN1))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=DAXX_ATRX))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=Grade))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MCT4_max))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), ncol =2, align = "v"
)
```
Again there seems to be no significant cluster. Maybe Beta-like samples are a little bit isolated 

```{r quantification of nromalization effect}

getNormQual = function(norm_data,
                       raw_data){
  # this function is taken from the wateRmelon package (qual)
  # The na.rm = T settings are removed to have the function give an error if there are NAs
  # difference between normalized and raw
  stopifnot(all.equal(colnames(norm_data), colnames(raw_data)))
  dif  = norm_data - raw_data
  # root mean square deviation
  rmsd = sqrt(colMeans(dif^2))
  sdd  = apply(dif, 2, sd)
  sadd = apply(abs(dif), 2, sd)
  srms = rmsd/sdd
  data.frame(Sample_Name = colnames(norm_data),
             rmsd,
             sdd,
             sadd,
             srms)
}
 
```

 

```{r normalization violence, fig.height = 3.5, fig.width = 4.5}
norm_qual <- getNormQual(meth_norm, meth_combat_beta)
#detach("package:factoextra", unload=TRUE)
require(ggrepel)
require(ggplot2)
cowplot::plot_grid(
  left_join(meta_data,
            norm_qual,
            by = "Sample_Name") %>%
    ggplot(aes(x = rmsd,
               y = sdd,
               label = Sample_Name,
               color= Technology)) +
    geom_point(size = 3,
               alpha = 0.75) +
    geom_text_repel() +
    scale_color_brewer(palette = "Set1") +
    theme_classic() +
    labs(title = "NOOB + BMIQ")
)

```

This is a final "check" to see if the technology used could have a bias in our data. Here we see that the EPIC has the same trend as the 450K indicating that there shouldn't be any bias (for now...). 

# Classifier development

The random forest was first trained on the entire data set with 10'000 trees. This allowed to look at the most important probes. The final classifier was then trained on the top 10'000 thousand probes only. 

However, according to some literature (https://doi.org/10.1007/978-3-642-31537-4_13 , https://doi.org/10.1002/widm.1301), 1'000 trees seem to be enough and the biggest performance gain can often be achieved when growing the first 100 trees. Since 10'000 trees take a lot of computation time, hyper parameter tuning could be useful. The function `rfcv` was then run by testing 100, 500, 1000, 5000 and 10000 trees on the top 10, 100, 1000, 10000 probes. 

![image](./results/fine_tuning.png)

There seem to be no difference between number of trees (row wise). However the number of probes has an impact. The following tests were then followed: 

* 10'000 probes, 10'000 trees
* 2'000 probes, 1'000 trees
* 1'000 probes 1'000 trees

We realized that the number of trees could be reduced to 500. We then tried to reduce the number of probes needed for the random forest. After testing with 10'000, 2'000 and 1'000 probes the model still seemed to be overfitting the date and the error rate didn't to go down much. We then decided to reduce to 10, 25, 50, 100, 200 and 500 probes :

![image](./results/n_probe_selection.png)


In order to select the ideal number of probes, we checked for confusion matrices, AUC and miss-classification rates of the 6 previous tests.

**NOTE** the files for the 10'000, 2'000 and 1'000 probes have been deleted but if needed, the can be run using the the `pipeline_5_classes.sh` script with the corresponding number of probes. The number of trees still remains 500. If you want more trees, please change it in the scripts. 

# Classifier cross-validation
Batch effect was NOT taken into account here !
```{r}
rm(list=ls())
load_file <- function(file=file){
  load(file = file, temp_env <- new.env())
  data <- as.list(temp_env)
  return(CV_result(data=data))
}

CV_result <- function(data=data){
  require(pROC)
  require(HandTill2001)
  source(file.path("scripts", "R","multi_brier.R"))
  source(file.path("scripts", "R","multi_logloss.R"))
  AUCscores <- HandTill2001::auc(multcap(response=as.factor(data$y),predicted=data$scores))
  AUCprobs <-  HandTill2001::auc(multcap(response=as.factor(data$y),predicted=data$probs))
  
  errs <- sum(data$y!=data$ys)/length(data$y)
  errp <- sum(data$y!=data$yp)/length(data$y)
  
  briers <- brier(data$scores,data$y)
  brierp <- brier(data$probs,data$y) 
  
  logls <- mlogloss(data$scores,data$y)
  loglp <- mlogloss(data$probs,data$y) 
  
  out <- cbind(c(AUCscores,AUCprobs),c(errs,errp),c(briers,brierp),c(logls,loglp))
  colnames(out) <- c("auc","misclassification","brier score","logloss")
  rownames(out) <- c("raw scores","calibrated scores")
  
  y.true <- data$y
  y.score <- data$ys
  y.calibrated <- data$yp
  return(list(AUCprobs=AUCprobs,
              AUCscores=AUCscores, errs=errs,
              errp=errp, briers=briers, brierp=brierp,
              logls=logls, loglp=loglp, out=out,
              y.true=y.true, y.score=y.score, y.calibrated=y.calibrated, 
              scores=data$scores,
              probs=data$probs))
}

confusion_matrix <- function(data=data,n_probes=n_probes){
  require(pheatmap)
  require(RColorBrewer)
  require(ggplot2)
  m <-table(data$y.true, data$y.calibrated)
  m <- m/apply(m, 1, sum)
  
  return(
  pheatmap(m,
           display_numbers = table(data$y.true, data$y.calibrated),
           fontsize_number=15,
           fontsize_row=15,
           fontsize_col = 15,
           angle_col = 45,
           treeheight_row = 0,
           treeheight_col = 0,
           cluster_rows = F,
           cluster_cols = F,
           color=colorRampPalette(c("white", "red"))(100),
           main=paste0("Confusion matrix of top ", n_probes," probes."))
  )
}


ROC_graph <- function(data=data, n_probes=n_probes){
  require(pROC)
  require(ggplot2)
  require(RColorBrewer)
  roc_resp <- as.numeric(data$y.true!=data$y.calibrated)
  predictor_roc <- apply(data$probs, 1, max)
  roc_to_plot <- roc(response=roc_resp, predictor=predictor_roc, ci=TRUE, of="se")
  roc_with_ci <- function(obj) {
    ciobj <- obj$ci
    dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                         lower = ciobj[, 1],
                         upper = ciobj[, 3])
    
    ggroc(obj, legacy.axes = T) +
      theme_minimal() +
      geom_abline(
        slope = 1,
        intercept = 0,
        linetype = "dashed",
        alpha = 0.7,
        color = "black"
      ) + coord_equal() +
      geom_ribbon(
        data = dat.ci,
        aes(x = 1-x, ymin = lower, ymax = upper),
        fill = "steelblue",
        alpha = 0.3
      ) + ggtitle(paste0("AUC of ", format(round(obj$auc, 2), nsmall = 2), " for top ", n_probes, " probes."))
  } 
  
  return(roc_with_ci(roc_to_plot))
}

calibrated_score_plot <- function(data=data){
  require(ggplot2)
  require(ggridges)
  calibrated_score <- as.data.frame(data$probs)
  calibrated_score <- calibrated_score[data$y.true == data$y.calibrated, ]
  max <- apply(calibrated_score, 1, max)
  Classes <- colnames(calibrated_score)[apply(calibrated_score, 1, which.max)]
  calibrated_score <- cbind(calibrated_score, max)
  calibrated_score <- cbind(calibrated_score, Classes)
  
  # plot the raw scores
  calibrated <- ggplot(calibrated_score, aes(x = max, y = Classes, fill = Classes)) +
    xlim(0,1.2) +
    ggtitle("Calibrated scores for correctly classified cases") +
    geom_density_ridges() +
    theme_ridges() + 
    theme(legend.position = "none")
  return(calibrated)
}

raw_score_plot <- function(data=data){
  require(ggplot2)
  require(ggridges)
  raw_score <- as.data.frame(data$scores)
  raw_score <- raw_score[data$y.true == data$y.score, ]
  max <- apply(raw_score, 1, max)
  Classes <- colnames(raw_score)[apply(raw_score, 1, which.max)]
  raw_score <- cbind(raw_score, max)
  raw_score <- cbind(raw_score, Classes)
  
  # plot the raw scores
  raw <- ggplot(raw_score, aes(x = max, y = Classes, fill = Classes)) +
    xlim(0,1.2) +
    ggtitle("Raw scores for correctly classified cases") +
    geom_density_ridges() +
    theme_ridges() + 
    theme(legend.position = "none")
  return(raw)
}

```

```{r load all files}
probes_10 <- load_file(file="./out_10_probes/CVresults.RData")
probes_25 <- load_file(file="./out_25_probes/CVresults.RData")
probes_50 <- load_file(file="./out_50_probes/CVresults.RData")
probes_100 <- load_file(file="./out_100_probes/CVresults.RData")
probes_200 <- load_file(file="./out_200_probes/CVresults.RData")
probes_500 <- load_file(file="./out_500_probes/CVresults.RData")

```

```{r, echo=FALSE, include=FALSE}
conf_mat_10 <- confusion_matrix(probes_10, 10)
conf_mat_25 <- confusion_matrix(probes_25, 25)
conf_mat_50 <- confusion_matrix(probes_50, 50)
conf_mat_100 <- confusion_matrix(probes_100, 100)
conf_mat_200 <- confusion_matrix(probes_200, 200)
conf_mat_500 <- confusion_matrix(probes_500, 500)
```

```{r, fig.height=20, fig.width=20}
library("grid")
library("gridExtra")
library("pheatmap")
library("ggplot2")

grid.arrange(grobs=list(conf_mat_10[[4]],
                        conf_mat_25[[4]],
                        conf_mat_50[[4]],
                        conf_mat_100[[4]],
                        conf_mat_200[[4]],
                        conf_mat_500[[4]]), ncol=2)
```

**For top 10 probes** 
```{r}
require(kableExtra)
probes_10$out %>% kbl %>% kable_styling()
```


**For top 25 probes**
```{r}
require(kableExtra)
probes_25$out %>% kbl %>% kable_styling()
```


**For top 50 probes **
```{r}
require(kableExtra)
probes_50$out %>% kbl %>% kable_styling()
```


**For top 100 probes **
```{r}
require(kableExtra)
probes_100$out %>% kbl %>% kable_styling()
```


**For top 200 probes **
```{r}
require(kableExtra)
probes_200$out %>% kbl %>% kable_styling()
```


**For top 500 probes **
```{r}
require(kableExtra)
probes_500$out %>% kbl %>% kable_styling()
```


```{r, fig.height=10, fig.width=14}
roc_10 <- ROC_graph(probes_10, 10)
roc_25 <- ROC_graph(probes_25, 25)
roc_50 <- ROC_graph(probes_50, 50)
roc_100 <- ROC_graph(probes_100, 100)
roc_200 <- ROC_graph(probes_200, 200)
roc_500 <- ROC_graph(probes_500, 500)
```

```{r, fig.height=10, fig.width=14}
cowplot::plot_grid(roc_10,
                   roc_25, 
                   roc_50,
                   roc_100,
                   roc_200,
                   roc_500,
                   ncol =3, align = "v")
```

```{r}
raw_10 <- raw_score_plot(probes_10)
raw_25 <- raw_score_plot(probes_25)
raw_50 <- raw_score_plot(probes_50)
raw_100 <- raw_score_plot(probes_100)
raw_200 <- raw_score_plot(probes_200)
raw_500 <- raw_score_plot(probes_500)
```

```{r}
calibrated_10 <- calibrated_score_plot(probes_10)
calibrated_25 <- calibrated_score_plot(probes_25)
calibrated_50 <- calibrated_score_plot(probes_50)
calibrated_100 <- calibrated_score_plot(probes_100)
calibrated_200 <- calibrated_score_plot(probes_200)
calibrated_500 <- calibrated_score_plot(probes_500)
```

```{r, fig.width=15, fig.height=20}
cowplot::plot_grid(raw_10, calibrated_10,
                   raw_25, calibrated_25,
                   raw_50, calibrated_50,
                   raw_100, calibrated_100,
                   raw_200, calibrated_200,
                   raw_500, calibrated_500,
  ncol = 2, labels = c(rep(c("10 probes", "25 probes", "50 probes", "100 probes", 
      "200 probes", "500 probes"),each=2)))
```



# Probes  in DiDomenico paper

```{r, echo=FALSE, include=T}
require(readxl)
alpha_vs_beta <-  read_xlsx(path="./data/raw_data/DiDomenico_2020_T6_alpha_like_v_beta_like.xlsx",
                           skip = 2)
alpha_vs_beta <- alpha_vs_beta[,1]
colnames(alpha_vs_beta) <- "probes"

alpha_vs_intermediate <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T7_alpha_like_v_intermediate.xlsx",
                                   skip = 2)
alpha_vs_intermediate <- alpha_vs_intermediate[,1]
colnames(alpha_vs_intermediate) <- "probes"

intermediate_vs_beta <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T8_intermediate_v_beta_like.xlsx",
                                  skip = 2)
intermediate_vs_beta <- intermediate_vs_beta[,1]
colnames(intermediate_vs_beta) <- "probes"
```

```{r}
require(dplyr)
require(randomForest)
load(file="./out_100_probes/rf.pred.RData")
probes <- as.data.frame(rownames(importance(rf.pred, type=1)))
colnames(probes) <- "probes"
final_probes <- unique(c(intersect(alpha_vs_beta$probes, probes$probes),
                  intersect(alpha_vs_intermediate$probes, probes$probes),
                  intersect(intermediate_vs_beta$probes, probes$probes)
)
)
```

```{r, echo=FALSE, include=T, message=FALSE}
require(ChAMP)
data("probe.features")
```

```{r}
require(kableExtra)
probe.features[final_probes,] %>% kbl %>% kable_styling()
```



