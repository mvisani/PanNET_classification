---
title: "043_panet_classification_5_classes"
author: "Marco Visani"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: true
    code_folding: hide
    df_print: kable
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE,warning = F)
```

# TL;DR (Too Long; Didn't Read)
* Training data taken from [_Di Domenico et al._ paper](https://doi.org/10.1038/s42003-020-01479-y) plus 30 additional samples.
* Code taken and modified from https://github.com/mwsill/mnp_training.
* Data was normalized and converted to beta values.
* QC performed.
* Model went from 10'000 trees and 10'000 probes to 500 trees and 100 probes. 
* Batch effect was NOT taken into account (and should be in the future as more data comes).
* Cross validation shows 20-30% error rate.
* Uncalibrated scores were kept because calibrated scores were increasing the error rate of the classes that were difficult to classify. 
* Only a fraction of the probes of the random forest model are DMPs.
* New data is predicted with 71% accuracy. 

If there are any questions, do not hesitate to contact me on [GitHub](https://github.com/mvisani/PanNET_classification) or [send me an email](mailto:marco.visani@unifr.ch). 

# Introduction
Data received was of RGChannelSetExtended formal class. NOOB was run for both 450 and EPIC data. In order to include as much data as possible, low p value probes and low bead count probes were also included. Legacy probes were removed (total of 997 probes). The probes were sorted by probe name. Data was converted to beta values. Taking the intersect between the probe names of EPIC and 450K technology and the then data was joined together. Beta values were then normalized using `champ.norm` function. 

Normalized beta values were converted back to M values. Data was adjusted for batch effect on the `Slide` using ComBat. Values were then converted back to beta values. 

QC was then performed (see below).  

For further details of preprocessing, please see the source code available on [GitHub](https://github.com/mvisani/PanNET_classification).

# Quality control 
Surrogate variables, PCA and t-SNE were checked. 

## Check for surrogate variables

First we checked for surrogate variables. Some were found but on the variables that we are interested in. If some of the surrogate variables were found on the `Technology` variable for example, this should be a concern. Here, there is nothing to worry about. 

```{r}
rm(list = ls())
meta_data <- read.table("./data/meta_data/training_meta_data.txt", sep = "\t", header = T)
if(!exists("meth_combat_M"))
  meth_combat_M <- readRDS("./data/results/meth_combat_M.Rds")
if(!exists("meth_combat_beta"))
  meth_combat_beta <- as.data.frame(readRDS("./data/results/meth_combat_beta.Rds"))
if (!exists("meth_norm"))
  meth_norm <- as.data.frame(readRDS("./data/results/meth_normalized.Rds"))
```

```{r, fig.width = 14}
load(file = "./data/results/surrogate_var.RData")
require(ggplot2)
require(dplyr)
#this part will tell if the surrogate variables are descibing other varibales of the dataset
methSV <- as.data.frame(meth_sva$sv)
colnames(methSV) <- paste0("S", colnames(methSV))
methSV <- cbind(meta_data,methSV) 

SVList = as.list(rep(NA, times = sum(grepl("SV", colnames(methSV)))))
for (sv in seq_along(SVList)){
  SVList[[sv]] = lapply(colnames(meta_data), function(var){
    if(is.numeric(methSV[[var]]))
      test_res = summary(lm(as.formula(paste0("SV", sv,"~", var)), data = methSV))$coef[2,4]
    else
      test_res = kruskal.test(as.formula(paste0("SV", sv,"~", var)), data = methSV)$p.value
    data.frame(variable = var,
               pval = test_res,
               SV = paste0("SV", sv))
  })
}

#This part plot the 
bind_rows(SVList) %>% 
  mutate(pval_bin = cut(-log10(pval), 
                        breaks = c(0, -log10(0.05), 2, 5, 10, 300), 
                        labels = c("a", "b", "c", "d", "e"), 
                        include.lowest = T), SV = factor(SV, levels=paste0("SV", 1:42))) %>% 
  ggplot(aes(x = SV, y = variable, fill = pval_bin)) + 
  geom_tile() + 
  coord_fixed() + 
  scale_fill_manual(values = c("white", "pink", "orange", "red", "darkred"),
                    limits = c("a", "b", "c", "d", "e"),
                    labels = c("p >= 0.05", "p < 0.05", "p < 0.01", 
                               expression("p < 1x" ~ 10^{-5}), 
                               expression("p < 1x" ~ 10^{-10}))) +
  theme_classic() + 
  theme(legend.title = element_blank(), 
        legend.text.align = 0, 
        legend.background = element_rect(fill = "grey90"), 
        axis.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) + 
  labs(title = "SVs vs biological variables")
```

Since we have many surrogate variables, there is nothing to worry about on the `Array` variable because the p-value is at 0.01 so there is a small percentage of chance that that SV was just assigned by chance. 

## PCA
PCA tries to preserve the global structure of data i.e when converting d-dimensional data to d'-dimensional data then it tries to map all the clusters as a whole due to which local structures might get lost. That is why we also checked with t-SNE. 
```{r, fig.height=10, fig.width=14}
require(fafreqs)
require(tibble)
#probevar <- rowVars(meth_combat_M)
probevar <- apply(meth_combat_M, 1, var)
probevar <- order(probevar, decreasing = T)

meth_combat_pca <- meth_combat_M[probevar[1:30000],meta_data$Sample_Name] #30000 is an arbitrary number --> look at paper on brain tumors (they took 32000)

meth_combat_pca <- t(meth_combat_pca)
pca_result <- prcomp(meth_combat_pca)

pca_df = pca_result$x %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample_Name")
pca_df = left_join(pca_df, 
                   meta_data,
                   by = "Sample_Name")
pca_eig = factoextra::get_eigenvalue(pca_result)



cowplot::plot_grid(
  ggplot(pca_df, aes(color=Technology, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MEN1, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=DAXX_ATRX, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=Grade, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=MCT4_max, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  
  ggplot(pca_df, aes(color=CC_Epi_newLRO, x=PC1, y=PC2))+
    geom_point()+labs(x=paste0("PC1 ", round(pca_eig$variance.percent[1], 2), "%"), 
                       y=paste0("PC2 ", round(pca_eig$variance.percent[2]), "%")),
  ncol =2, align = "v" 
)

```

We can't see any pattern. Maybe a small one for the Intermidiate_ADM probes classified by the `CC_Epi_newLRO` (bottom left image). 
The two first PCs only describe in total 21\% of the variance, which is quite low.

## t-SNE
```{r, fig.height=10, fig.width=14}
require(Rtsne)
require(RColorBrewer)
set.seed(123) # setting a fixed seed allows reproducibility 
tsne <- Rtsne::Rtsne(meth_combat_pca, theta=0.0, PCA=F,
                     max_iter=2500) #with default perplexity of 30. Theta set to 0.0 for higher accuracy (but lower speed)
                                    # PCA and max_iter are set as in Capper et al. paper
tsne_df <- tsne$Y %>% as.data.frame() %>% rownames_to_column("Sample_Name")
tsne_df$Sample_Name <- c(rownames(meth_combat_pca))
tsne_df <- left_join(tsne_df, meta_data, by="Sample_Name")


cowplot::plot_grid(
  ggplot(tsne_df, aes(x=V1, y=V2, color=Technology))+
  xlab("tSNE1") + ylab("tSNE2")+
    geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=CC_Epi_newLRO))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MEN1))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=DAXX_ATRX))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=Grade))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), 
  
  ggplot(tsne_df, aes(x=V1, y=V2, color=MCT4_max))+
    xlab("tSNE1") + ylab("tSNE2")+
  geom_point(), ncol =2, align = "v"
)
```
Again there seems to be no significant cluster. Maybe Beta-like samples are a little bit isolated. However, the main concern was on technology and as we can see, it doesn't seem to have an impact. 

```{r quantification of nromalization effect}

getNormQual = function(norm_data,
                       raw_data){
  # this function is taken from the wateRmelon package (qual)
  # The na.rm = T settings are removed to have the function give an error if there are NAs
  # difference between normalized and raw
  stopifnot(all.equal(colnames(norm_data), colnames(raw_data)))
  dif  = norm_data - raw_data
  # root mean square deviation
  rmsd = sqrt(colMeans(dif^2))
  sdd  = apply(dif, 2, sd)
  sadd = apply(abs(dif), 2, sd)
  srms = rmsd/sdd
  data.frame(Sample_Name = colnames(norm_data),
             rmsd,
             sdd,
             sadd,
             srms)
}
 
```

 

```{r normalization violence, fig.height = 3.5, fig.width = 4.5}
norm_qual <- getNormQual(meth_norm, meth_combat_beta)
#detach("package:factoextra", unload=TRUE)
require(ggrepel)
require(ggplot2)
cowplot::plot_grid(
  left_join(meta_data,
            norm_qual,
            by = "Sample_Name") %>%
    ggplot(aes(x = rmsd,
               y = sdd,
               label = Sample_Name,
               color= Technology)) +
    geom_point(size = 3,
               alpha = 0.75) +
    geom_text_repel() +
    scale_color_brewer(palette = "Set1") +
    theme_classic() +
    labs(title = "NOOB + BMIQ")
)

```

This is a final "check" to see if the technology used could have a bias in our data. Here we see that the EPIC has the same trend as the 450K indicating that there shouldn't be any bias (for now...). 

# Classifier development

In the first version of the project, the random forest was first trained on the entire data set with 10'000 trees. This allowed to look at the most important probes. The final classifier was then trained on the top 10'000 thousand probes only. 

However, according to some literature (https://doi.org/10.1007/978-3-642-31537-4_13 , https://doi.org/10.1002/widm.1301), 1'000 trees seem to be enough and the biggest performance gain can often be achieved when growing the first 100 trees. Since 10'000 trees take a lot of computation time, hyper parameter tuning could be useful. The function `rfcv` was then run by testing 100, 500, 1000, 5000 and 10000 trees on the top 10, 100, 1000, 10000 probes. 

![Fine tuning the number of trees](./results/image/5_classes/tree_selection_5_classes.png)

There seem to be no difference between number of trees (row wise). However the number of probes has an impact. The following tests were then followed: 

* 10'000 probes, 10'000 trees
* 2'000 probes, 1'000 trees
* 1'000 probes 1'000 trees

We realized that the number of trees could be reduced to 500. We then tried to reduce the number of probes needed for the random forest. After testing with 10'000, 2'000 and 1'000 probes the model still seemed to be overfitting the data and the error rate didn't to go down much. We then decided to reduce to 10, 25, 50, 100, 200 and 500 probes :

![Fine tuning the number of probes](./results/image/5_classes/probe_slection_5_classes.png)

In order to select the ideal number of probes, we checked for confusion matrices, AUC and miss-classification rates of the 6 previous tests.

**NOTE** the files for the 10'000, 2'000 and 1'000 probes have been deleted but if needed, the can be run using the the `pipeline_5_classes.sh` script with the corresponding number of probes. The number of trees still remains 500. If you want more trees, please change it in the scripts. 

# Classifier cross-validation
**Attention** Batch effect was NOT taken into account here ! The function in the scripts that removes batch effect has been modified. If you want to add batch effect, the function has to be modified. Please see https://github.com/mwsill/mnp_training/blob/master/R/calculateCVfold.R and https://github.com/mwsill/mnp_training/blob/master/R/batchadjust.R


```{r}
rm(list=ls())
load_file <- function(file=file){
  load(file = file, temp_env <- new.env())
  data <- as.list(temp_env)
  return(CV_result(data=data))
}

CV_result <- function(data=data){
  require(pROC)
  require(HandTill2001)
  source(file.path("scripts", "R","multi_brier.R"))
  source(file.path("scripts", "R","multi_logloss.R"))
  AUCscores <- HandTill2001::auc(multcap(response=as.factor(data$y),predicted=data$scores))
  AUCprobs <-  HandTill2001::auc(multcap(response=as.factor(data$y),predicted=data$probs))
  
  errs <- sum(data$y!=data$ys)/length(data$y)
  errp <- sum(data$y!=data$yp)/length(data$y)
  
  briers <- brier(data$scores,data$y)
  brierp <- brier(data$probs,data$y) 
  
  logls <- mlogloss(data$scores,data$y)
  loglp <- mlogloss(data$probs,data$y) 
  
  out <- cbind(c(AUCscores,AUCprobs),c(errs,errp),c(briers,brierp),c(logls,loglp))
  colnames(out) <- c("auc","misclassification","brier score","logloss")
  rownames(out) <- c("raw scores","calibrated scores")
  
  y.true <- data$y
  y.score <- data$ys
  y.calibrated <- data$yp
  return(list(AUCprobs=AUCprobs,
              AUCscores=AUCscores, errs=errs,
              errp=errp, briers=briers, brierp=brierp,
              logls=logls, loglp=loglp, out=out,
              y.true=y.true, y.score=y.score, y.calibrated=y.calibrated, 
              scores=data$scores,
              probs=data$probs))
}

confusion_matrix <- function(data=data,n_probes=n_probes){
  require(pheatmap)
  require(RColorBrewer)
  require(ggplot2)
  m <-table(data$y.true, data$y.calibrated)
  m <- m/apply(m, 1, sum)
  
  return(
  pheatmap(m,
           display_numbers = table(data$y.true, data$y.calibrated),
           fontsize_number=15,
           fontsize_row=15,
           fontsize_col = 15,
           angle_col = 45,
           treeheight_row = 0,
           treeheight_col = 0,
           cluster_rows = F,
           cluster_cols = F,
           color=colorRampPalette(c("white", "red"))(100),
           main=paste0("Confusion matrix of top ", n_probes," probes."))
  )
}


ROC_graph <- function(data=data, n_probes=n_probes){
  require(pROC)
  require(ggplot2)
  require(RColorBrewer)
  roc_resp <- as.numeric(data$y.true!=data$y.calibrated)
  predictor_roc <- apply(data$probs, 1, max)
  roc_to_plot <- roc(response=roc_resp, predictor=predictor_roc, ci=TRUE, of="se")
  roc_with_ci <- function(obj) {
    ciobj <- obj$ci
    dat.ci <- data.frame(x = as.numeric(rownames(ciobj)),
                         lower = ciobj[, 1],
                         upper = ciobj[, 3])
    
    ggroc(obj, legacy.axes = T) +
      theme_minimal() +
      geom_abline(
        slope = 1,
        intercept = 0,
        linetype = "dashed",
        alpha = 0.7,
        color = "black"
      ) + coord_equal() +
      geom_ribbon(
        data = dat.ci,
        aes(x = 1-x, ymin = lower, ymax = upper),
        fill = "steelblue",
        alpha = 0.3
      ) + ggtitle(paste0("AUC of ", format(round(obj$auc, 2), nsmall = 2), " for top ", n_probes, " probes."))
  } 
  
  return(roc_with_ci(roc_to_plot))
}

calibrated_score_plot <- function(data=data){
  require(ggplot2)
  require(ggridges)
  calibrated_score <- as.data.frame(data$probs)
  calibrated_score <- calibrated_score[data$y.true == data$y.calibrated, ]
  max <- apply(calibrated_score, 1, max)
  Classes <- colnames(calibrated_score)[apply(calibrated_score, 1, which.max)]
  calibrated_score <- cbind(calibrated_score, max)
  calibrated_score <- cbind(calibrated_score, Classes)
  
  # plot the raw scores
  calibrated <- ggplot(calibrated_score, aes(x = max, y = Classes, fill = Classes)) +
    xlim(0,1.2) +
    ggtitle("Calibrated scores for correctly classified cases") +
    geom_density_ridges() +
    theme_ridges() + 
    theme(legend.position = "none")
  return(calibrated)
}

raw_score_plot <- function(data=data){
  require(ggplot2)
  require(ggridges)
  raw_score <- as.data.frame(data$scores)
  raw_score <- raw_score[data$y.true == data$y.score, ]
  max <- apply(raw_score, 1, max)
  Classes <- colnames(raw_score)[apply(raw_score, 1, which.max)]
  raw_score <- cbind(raw_score, max)
  raw_score <- cbind(raw_score, Classes)
  
  # plot the raw scores
  raw <- ggplot(raw_score, aes(x = max, y = Classes, fill = Classes)) +
    xlim(0,1.2) +
    ggtitle("Raw scores for correctly classified cases") +
    geom_density_ridges() +
    theme_ridges() + 
    theme(legend.position = "none")
  return(raw)
}

```

```{r load all files}
probes_10 <- load_file(file="./out_10_probes/CVresults.RData")
probes_25 <- load_file(file="./out_25_probes/CVresults.RData")
probes_50 <- load_file(file="./out_50_probes/CVresults.RData")
probes_100 <- load_file(file="./out_100_probes/CVresults.RData")
probes_200 <- load_file(file="./out_200_probes/CVresults.RData")
probes_500 <- load_file(file="./out_500_probes/CVresults.RData")

```

```{r, echo=T, include=F}
conf_mat_10 <- confusion_matrix(probes_10, 10)
conf_mat_25 <- confusion_matrix(probes_25, 25)
conf_mat_50 <- confusion_matrix(probes_50, 50)
conf_mat_100 <- confusion_matrix(probes_100, 100)
conf_mat_200 <- confusion_matrix(probes_200, 200)
conf_mat_500 <- confusion_matrix(probes_500, 500)
```

## Confusion matrices


```{r, fig.height=20, fig.width=20}
library("grid")
library("gridExtra")
library("pheatmap")
library("ggplot2")

grid.arrange(grobs=list(conf_mat_10[[4]],
                        conf_mat_25[[4]],
                        conf_mat_50[[4]],
                        conf_mat_100[[4]],
                        conf_mat_200[[4]],
                        conf_mat_500[[4]]), ncol=2)
```

Shows confusion matrix as heatmap. Numbers in the cells indicates the number of samples counted in that cell. The color indicates the percentage of samples correctly identified in that class. 

Should be read from right to left. For example, if we take the top left image, there are 21 Alpha_like  that were classified as Intermidiate_ADM. 

We can see that the number of probes doesn't increase much the accuracy and the pattern is always more or less the same meaning that increasing the number of probes doesn't decrease the error rate of our model. 

## Misclassification rates

**For top 10 probes** 
```{r}
require(kableExtra)
probes_10$out %>% kbl %>% kable_styling()
```


**For top 25 probes**
```{r}
require(kableExtra)
probes_25$out %>% kbl %>% kable_styling()
```


**For top 50 probes **
```{r}
require(kableExtra)
probes_50$out %>% kbl %>% kable_styling()
```


**For top 100 probes **
```{r}
require(kableExtra)
probes_100$out %>% kbl %>% kable_styling()
```


**For top 200 probes **
```{r}
require(kableExtra)
probes_200$out %>% kbl %>% kable_styling()
```


**For top 500 probes **
```{r}
require(kableExtra)
probes_500$out %>% kbl %>% kable_styling()
```

AUCs were ignored in this table because there seem to be a bias due to the lack of data. 

Missclassification rates remain more or less stable starting from 50 probes or more. This means once again that the increasing the number of probes doesn't reduce the missclassification rate of our model. 

## AUC curves

```{r, fig.height=10, fig.width=14}
roc_10 <- ROC_graph(probes_10, 10)
roc_25 <- ROC_graph(probes_25, 25)
roc_50 <- ROC_graph(probes_50, 50)
roc_100 <- ROC_graph(probes_100, 100)
roc_200 <- ROC_graph(probes_200, 200)
roc_500 <- ROC_graph(probes_500, 500)
```

```{r, fig.height=10, fig.width=14}
cowplot::plot_grid(roc_10,
                   roc_25, 
                   roc_50,
                   roc_100,
                   roc_200,
                   roc_500,
                   ncol =3, align = "v")
```

AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. 

Again, no big difference between the different number of probes. We constantly seem to remain at an error rate of 20%. This is probably due to the fact that our response isn't as precise as we would like it to be. 

## Score calibration 

```{r}
raw_10 <- raw_score_plot(probes_10)
raw_25 <- raw_score_plot(probes_25)
raw_50 <- raw_score_plot(probes_50)
raw_100 <- raw_score_plot(probes_100)
raw_200 <- raw_score_plot(probes_200)
raw_500 <- raw_score_plot(probes_500)
```

```{r}
calibrated_10 <- calibrated_score_plot(probes_10)
calibrated_25 <- calibrated_score_plot(probes_25)
calibrated_50 <- calibrated_score_plot(probes_50)
calibrated_100 <- calibrated_score_plot(probes_100)
calibrated_200 <- calibrated_score_plot(probes_200)
calibrated_500 <- calibrated_score_plot(probes_500)
```

```{r, fig.width=15, fig.height=20}
cowplot::plot_grid(raw_10, calibrated_10,
                   raw_25, calibrated_25,
                   raw_50, calibrated_50,
                   raw_100, calibrated_100,
                   raw_200, calibrated_200,
                   raw_500, calibrated_500,
  ncol = 2, labels = c(rep(c("10 probes", "25 probes", "50 probes", "100 probes", 
      "200 probes", "500 probes"),each=2)))
```

From here we decided that 100 probes would be the best fit. To avoid overfitting, we decided to stay parsimonious and keep the lowest possible amount of probes. Unfortunately, calibration didn't seemed not to increase the scores very well. This is probably due to the amount of data we have. The classes with enough data or classes that were very different form the rest of the dataset (Intermediate_ADM and Beta_like) could have their score increased but the others couldn't. 

In the future, if more data is added, calibration will have to be performed but right now it doesn't help our model. We decided that for predicting new data. We would keep the **uncalibrated score** for know. 


## Consensus clustering

```{r}
require(pheatmap)
cons_cluster <- readRDS(file = "./data/raw_data/220609_NDD.consClust_DMP.Rds")
cons_cluster <- cons_cluster[[5]]
```

```{r}
plotCCheatmap = function(ccObject, metaData, previousCC = NULL, colorList){
  ccHmap = ccObject$consensusMatrix
  ccDist = ccObject$consensusTree
  ccDist_rev = ccDist
  ccDist_rev$order = rev(ccDist_rev$order)
  
  attr(ccHmap, "dimnames") = list(names(ccObject$consensusClass),
                                names(ccObject$consensusClass))
  
  metaData = as.data.frame(metaData)
  if(all.equal(rownames(metaData), as.character(seq_len(nrow(metaData)))))
    rownames(metaData) = metaData$Sample_Name
  metaData = metaData %>% 
    dplyr::select(-Sample_Name) 
  if(!is.null(previousCC))
    metaData[[previousCC]][is.na(metaData[[previousCC]])] = "new_sample"
  
  pheatmap(ccHmap,
           scale = "none",
           color = colorRampPalette(c("white", "blue"))(100),
           cluster_rows = ccDist_rev,
           cluster_cols = ccDist,
           show_rownames = F,
           show_colnames = F,
           cellheight = 2,
           cellwidth = 2,
           annotation_col = metaData,
           annotation_colors = colorList)
}
```

```{r, fig.height = 10, fig.width = 14}
require(dplyr)
meta_data <- read.table(file = "./data/meta_data/training_meta_data_new.txt", sep = "\t", header = T)

scores_plot <- probes_100$scores[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(scores_plot)[apply(scores_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(scores_plot)[apply(scores_plot, 1, which.max)]")] <- "rf"

#now for the calibrated scores
probs_plot <- probes_100$probs[meta_data$Sample_Name, ]
meta_data <- cbind(meta_data, colnames(probs_plot)[apply(probs_plot,1,which.max)])
colnames(meta_data)[which(names(meta_data) == "colnames(probs_plot)[apply(probs_plot, 1, which.max)]")] <- "rf_calibrated"


max_score <- apply(probes_100$probs[meta_data$Sample_Name, ], 1, max)
meta_data <- cbind(meta_data, max_score)

#load(file="./results/k_means_clustering.RData")
#colnames(groups) <- "k_mean"
#meta_data <- cbind(meta_data, groups[meta_data$Sample_Name,])
#colnames(meta_data)[which(names(meta_data) == "groups[meta_data$Sample_Name, ]")] <- "k_mean"
#meta_data$k_mean <- as.factor(meta_data$k_mean)

plotCCheatmap(cons_cluster, 
              metaData = meta_data %>% 
                dplyr::select(Sample_Name, 
                              DAXX_ATRX, 
                              MEN1, 
                              CC_Epi_newLRO,
                              rf,
                              rf_calibrated, 
                              max_score),

              colorList = list(DAXX_ATRX = c(wt = "white", mut = "goldenrod4"),
                               MEN1 = c(wt = "white", mut = "goldenrod1"),
                               ConsClus_k4_UB_UCL_ICGC_Chan = c(Alpha_like = "dodgerblue4",
                                                                Beta_like = "green",
                                                                Intermediate_ADM = "lightskyblue1",
                                                                Intermediate_WT = "darksalmon",
                                                                new_sample = "yellow"),
                               CC_Epi_newLRO = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               rf = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"), 
                               rf_calibrated = c(Alpha_like = "dodgerblue4",
                                                 Beta_like = "green",
                                                 Intermediate_ADM = "lightskyblue1",
                                                 Intermediate_ADM_WT = "hotpink3",
                                                 Intermediate_WT = "darksalmon"),
                               max_score = colorRampPalette(c("white", "blue"))(100)))



```

Consensus cluster with 5 classes plotted with the results of the RF calibrated and uncalibrated. There seem to be a lot of disagreement between the consensus cluster and the RF. However, looking at the image above, we see that most disagreements are on samples that are not perfectly classified and that overlap with other classes. 

The advantage of the random forest is that it allows to see the probability of a sample to be in a specific class. 

### Samples difficult to classify

We then checked for the samples that were difficult to classify. To do so, we selected the maximal score uncalibrated of each sample ( _i.e._ the class assigned by the random forest) and looked at if there was another score that was close to that maximimal score (5% of the trees or less, see code below). This would allow to find the samples that are difficult to classify and to be able to further investigate those samples or train the random forest without those samples. 

```{r}
find_maximal_value <- function(row, threshold) {
  # Find maximal value in the row
  max_value <- max(row)
  
  # Compare other cells to the maximal value
  res <- (abs(row - max_value) <= threshold) & (abs(row - max_value) > 0.0)
  
  return(res)
}


# Apply the custom function to each row of the dataframe
test <- apply(scores_plot, 1, find_maximal_value, threshold = 0.05)
test <- t(test)

#dim(test[apply(test, 1, function(x) sum(x)!=0),])
print(rownames(test[apply(test, 1, function(x) sum(x)!=0),]))
```

Interestingly the number of samples difficult to classify are in the same range as the missclassification rate of the random forest. 

# Probes  in DiDomenico paper

We then wanted to see if there were any probes of the random forest that were also present in the [_DiDomenico et al._ paper](https://doi.org/10.1038/s42003-020-01479-y). The plot below shows the fraction of DMP of the paper that are present in the random forest model.

```{r, echo=FALSE, include=T}
require(readxl)
alpha_vs_beta <-  read_xlsx(path="./data/raw_data/DiDomenico_2020_T6_alpha_like_v_beta_like.xlsx",
                           skip = 2)
alpha_vs_beta <- alpha_vs_beta[,1]
colnames(alpha_vs_beta) <- "probes"

alpha_vs_intermediate <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T7_alpha_like_v_intermediate.xlsx",
                                   skip = 2)
alpha_vs_intermediate <- alpha_vs_intermediate[,1]
colnames(alpha_vs_intermediate) <- "probes"

intermediate_vs_beta <- read_xlsx(path = "./data/raw_data/DiDomenico_2020_T8_intermediate_v_beta_like.xlsx",
                                  skip = 2)
intermediate_vs_beta <- intermediate_vs_beta[,1]
colnames(intermediate_vs_beta) <- "probes"
```

```{r}
require(dplyr)
require(randomForest)
frac <- c()
for(i in c(10, 25, 50, 100, 200, 500, 1000)){
  load(file=paste0("./out_",i,"_probes/rf.pred.RData"))
  probes <- as.data.frame(rownames(importance(rf.pred, type=1)))
  colnames(probes) <- "probes"
  final_probes <- unique(c(intersect(alpha_vs_beta$probes, probes$probes),
                  intersect(alpha_vs_intermediate$probes, probes$probes),
                  intersect(intermediate_vs_beta$probes, probes$probes)))
  frac <- append(frac, length(final_probes)/i)
                  
} 
plot(c(10, 25, 50, 100, 200, 500, 1000), frac, type="o",
     ylim=c(0.0, 0.18),
     log="x",
     ylab="Fraction of DMP present in RF",
     xlab="Number of probes in the RF")
```

We can see that the fraction of DMP gradually decreases as the number of probes in the random forest increases. This could mean that there is probably a fixed number of DMP that are important for the random forest and that it always takes into account for the classification. 

The table below shows the 9 DMPs that are present also in the random forest for the 100 probes model. 

```{r}
require(dplyr)
require(randomForest)
load(file="./out_100_probes/rf.pred.RData")
probes <- as.data.frame(rownames(importance(rf.pred, type=1)))
colnames(probes) <- "probes"
final_probes <- unique(c(intersect(alpha_vs_beta$probes, probes$probes),
                  intersect(alpha_vs_intermediate$probes, probes$probes),
                  intersect(intermediate_vs_beta$probes, probes$probes)
)
)
```

```{r, echo=FALSE, include=T, message=FALSE}
require(ChAMP)
data("probe.features")
probe.features <- probe.features[, 1:11]
```

```{r}
require(kableExtra)
probe.features[final_probes,] %>% kbl %>% kable_styling()
```

The probes that is assigned to CASZ1 continuously comes back for every model. The gene is a Zinc finger. It might be worth looking into more details to these probes.

# Predicting new data

Once we had our model, we tried to predict the classes of some new samples. The new data had first removed of batch effect, be normalized and converted to beta values (as for the pre-processing of our model; script used was `00_normalization.R`). Then only the probes present in our model were kept and the data was predicted. 

```{r}
rm(list=ls())
require(kableExtra)
require(randomForest)
require(dplyr)

test_beta <- readRDS(file = "./data/results/test_beta.Rds")
meta_data <- read.table(file="./data/meta_data/test_EPIC_meta_data.txt", sep = "\t", header = T)
load(file="./out_100_probes/rf.pred.RData")
beta <- t(test_beta)
probes <- rownames(importance(rf.pred, importance=1))
beta <- beta[, probes]
answer <- predict(rf.pred, beta, type = "prob")

prediction <- colnames(answer)[apply(answer, 1, which.max)]
answer <- cbind(answer, prediction)


CC_epi <- meta_data$CC_Epi_newG1G2_individual
answer <- cbind(answer, CC_epi)

answer <- as.data.frame(answer)

print(sum(answer$prediction==answer$CC_epi)/dim(answer)[1])
```

We arrive at 71% accuracy (or agreement with the consensus clustering). Finally, we looked at which probes didn't agree with the consensus clustering:

```{r}
answer[answer$prediction!=answer$CC_epi,] %>% kbl %>% kable_styling()
```

We can see that there are a couple of probes where the random forest is hesitating between two classes. This could mean that there are maybe more classes than expected. 

It could be interesting to see if increasing the number of classes makes the random forest more accurate. However this involves the need of more data which for now we don't have. 

Another possibility would be to increase the number of trees in the random forest. As we have seen before, this is not the most useful however, it could help in some cases. 

# Outlook
As we have seen, PanNETs are very difficult to classify. The lack of data and the accuracy of our response made it difficult to achieve results that could be used clinically. The main drawback right now is the lack of data. In fact, machine learning algorithms often need a lot of data in order to have a high precision. 

Since PanNETS are so heterogeneous, we could imagine to change the response from a classification to a regression. Instead of having classes, we could some sort of score that represents the "gravity" of the tumor. By setting up a threshold (or multiple thresholds) one could maybe classify the tumors in another way. 